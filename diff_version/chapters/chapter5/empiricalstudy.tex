\section{Methodology} \label{sec:studysettings}
%We present two studies in this chapter: a quantitative and a qualitative study
%(\hyperref[st:study3]{Studies}~\ref{st:study3} and \ref{st:study4},
%respectively).  

In \hyperref[st:study3]{Study}~\ref{st:study3}, we set out to comparatively
analyze the delivery delay \DIFdelbegin \DIFdel{of }\DIFdelend \DIFaddbegin \DIFadd{in terms of days (see
}\hyperref[def:2]{Definition}\DIFadd{~\ref{def:2}) of }\DIFaddend addressed issues that were shipped
in traditional releases versus the ones that were shipped in rapid releases. In
this section, we provide information about the subject projects, data collection
process, and how we perform the analyses of our study.

\subsection{Subjects}

We choose to study the Firefox project because it offers a unique opportunity to
investigate the impact of shifting from a traditional release cycle to a rapid
release cycle using rich, publicly available ITS and \textit{Version Control
System} (VCS) data. Although other open source projects may have ITS and VCS
data available, they do not provide the opportunity to investigate the
transition between traditional releases and rapid releases. In addition,
comparing different projects that use traditional and rapid releases poses a
great challenge, since one has to distinguish to what extent the results are due
to the release strategy and not due to intricacies of the projects themselves.
Therefore, we highlight that the choice to investigate Firefox is not
accidental, but based on the specific analysis constraints that such data
satisfies, and the very unique nature of such data.

\subsection{Data Collection}\label{ch5:datacollection}

\hyperref[fig:database_construction]{Figure}~\ref{fig:database_construction}
shows an overview of our data collection approach. Each step of the process is
described below.

\begin{figure}[!]
	\centering
	\includegraphics[width=0.90\textwidth,keepaspectratio]
	{chapters/chapter5/figures/database_construction_final.pdf}
	\caption{
		Overview of the process to construct the dataset that is used in
		our \hyperref[st:study3]{Study}~\ref{st:study3}.
	}
	\label{fig:database_construction}
\end{figure}

\begin{table}[!]
	\footnotesize
	\centering
	\caption{The studied traditional and rapid Firefox releases.
	\label{tbl:releases}}
		\begin{tabular}{ccccc}
			\hline 
			\textbf{Strategy} & \textbf{Version range} &
			\textbf{Time period} & \textbf{\# of Majors} &
			\textbf{\# of Minors}\tabularnewline
			\hline 
			\hline 
			Trad. & 1.0 - 4.0 & Sep/2004 - Mar/2012 & 7 & 104\tabularnewline
			\hline 
			Rapid & 5 - 27 & Jun/2011 - Sep/2014 & 23 & 50\tabularnewline
			\hline 
		\end{tabular}%
	%	}
\end{table}

\noindent\textbf{\textit{Step 1: Collect release information.}} We collect the date and
version number of each Firefox release (minor and major releases of each release
strategy) using the Firefox release history
wiki.\smartfoot{\url{https://en.wikipedia.org/wiki/Firefox_release_history}}
\hyperref[tbl:releases]{Table}~\ref{tbl:releases} shows: {\em (i)}the range of versions of releases that we
investigate, {\em (ii)} the investigated time period of each release strategy, and
(iii) the number of major and minor studied releases in each release strategy.\\

\noindent\textbf{\textit{Step 2: Link issues to releases.}} Once we collect the release
information, we use the \textit{tags} within the VCS to link issue IDs to
releases.  First, we analyze the tags that are recorded within the VCS. Since
Firefox migrated from CVS to Mercurial during release 3.5, we collect the tags
of releases 1.0 to 3.0 from CVS, while we collect the tags of releases 3.5 to 27
from
Mercurial.\smartfoot{\url{http://cvsbook.red-bean.com/cvsbook.html}}$^,$\smartfoot{\url{https://mercurial.selenic.com/}}
By analyzing the tags, we extract the commit logs within each tag. The extracted
commit logs are linked to their respective tags. We then parse the commit logs
to collect the issue IDs that are being addressed in the commits. We discard the
following patterns of potential issue IDs that are false positives:

\begin{enumerate}
\item Potential IDs that have less than five digits, since the issue IDs of the
	investigated releases should have at least five digits (2,559 issues
	were discarded).  
\item Commit logs that follow the pattern: ``Bug $<$ID$>$ -
	reftest'' or ``Bug $<$ID$>$ - JavaScript Tests'',
	which refer to tests and not bug fixes (269 issues were
	discarded).  
\item Any potential ID that is the name of a file, \eg
	``159334.js'' (607 issues were discarded).
\end{enumerate}          

We find that all of the remaining IDs match issue IDs that exist in the Firefox
ITS.

Since the commit logs are linked to VCS tags, we are also able to link the
issue IDs found within these commit logs to the releases that correspond to
those tags. For example, since we find the fix for issue 529404 in the commit
log of tag 3.7a1, we link this issue~ID to that release. We also merge together
the data of development releases like 3.7a1 into the nearest minor or major
release. For example, release 3.7a1 would be merged with release 4.0, since it
is the next user-intended release after 3.7a1. In the case that a particular
issue is found in the commit logs of multiple releases, we consider that
particular issue to pertain to the earliest release that contains the last fix
attempt (commit log), since that release is the first one to contain the
complete fix for the issue. Finally, we collect the issue report information
of each remaining issue (\eg opening date, fix date, severity, priority, and
description) using the ITS. Moreover, since the minor-rapid releases are
\textit{off-cycle releases}, in which addressed issues may skip being integrated
into \code{mozilla-central} (\ie NIGHTLY) tags, we manually collect the addressed
issues that were integrated into those releases using the Firefox release notes
(\ie 247 addressed~issues).\smartfoot{\url{https://www.mozilla.org/en-US/firefox/releases/}}
We add the manually collected addressed issues from ESR releases within the rapid
releases data, since they also represent data from a rapid release strategy.\\

\noindent\textbf{\textit{Steps 3 and 4: Compute metrics and perform analyses.}}
We use the data from Step 2 to compute the metrics that we use in our analyses.
We select these metrics (which are described in the
\hyperref[ch5:rq3]{approach for RQ3}) because we suspect that they
share a relationship with delivery delay.

\section{Results} \label{ch4:results}

In this study, we address three research
questions about the shift from a traditional to a rapid release cycle. The
motivation of each research question is detailed below.

\subsection{RQ1: Are addressed issues delivered more quickly in rapid
releases?}\label{ch5:rq1} 

\subsubsection*{RQ1: Motivation} 

Since there is a lack of empirical evidence to indicate that rapid release
cycles deliver addressed issues more quickly than traditional release cycles, we
compare the delivery delay of addressed issues in traditional releases against
the delivery delay in rapid releases in \hyperref[ch5:rq1]{RQ1}.\\

\subsubsection*{RQ1: Approach}

\begin{figure}[t!]
	\centering
	\includegraphics[width=\columnwidth,keepaspectratio]
	{chapters/chapter5/figures/rq1/issue_lifecycle.pdf}
	\caption{A simplified life cycle of an issue.}
	\label{fig:issue_lifecycle}
\end{figure}

\hyperref[fig:issue_lifecycle]{Figure}~\ref{fig:issue_lifecycle} shows a
simplified life cycle of an issue, which includes the triaging phase ({\em t1}),
the fixing phase ({\em t2}), and the integration phase ({\em t3}). We consider
the last RESOLVED-FIXED status as the moment at which a particular issue was
addressed (the fixed state in
\hyperref[fig:issue_lifecycle]{Figure}~\ref{fig:issue_lifecycle}). The
\textit{lifetime} of an issue is composed of all three phases (from \textit{new}
to \textit{released}). For \hyperref[ch5:rq1]{RQ1}, we first observe the lifetime of the issues of
traditional and rapid releases.  Next, we look at the time span of the
\textit{triaging}, \textit{fixing}, and \textit{integration} phases within the
lifetime of an issue.

We use beanplots~\cite{kampstra2008beanplot} to compare the distributions of our
data. The vertical curves of beanplots summarize and compare the distributions
of different datasets (see
\hyperref[fig:delivery_delay]{Figure}~\ref{fig:delivery_delay}). The higher
the frequency of data within a particular value, the thicker the bean is plotted
at that particular value on the $y$ axis. We also use Mann-Whitney-Wilcoxon
(MWW) tests~\cite{wilks2011statistical} and Cliff's delta effect-size
measures~\cite{cliff1993dominance}. MWW tests are non-parametric tests of the
\textit{null hypothesis} that two distributions come from the same population
($\alpha=0.05$). On the other hand, Cliff's delta is a non-parametric
effect-size measure to verify the difference in magnitude of one distribution compared
to another distribution. The higher the value of the Cliff's delta,
the greater the difference of values between distributions. For instance, if we
obtain a significant $p$ value but a small Cliff's delta, this means that
although two distributions do not come from the same population their 
difference is not that large. A positive Cliff's delta indicates how much
larger the values of the first distribution are, while a negative Cliff's delta
indicates the inverse. Finally, we use the \textit{Median Absolute Deviation}
(MAD) \cite{howell2005median,leys2013detecting} as a measure of the variation of
our distributions. The MAD is the median of the \textit{absolute deviations}
from one distribution's median. The higher the MAD, the greater is the variation
of a distribution with respect to its median.

\subsubsection*{RQ1: Results}

\begin{figure}[t!]
	\centering
	\subfloat[Lifetime]{
		\includegraphics[width=.45\columnwidth,keepaspectratio]
		{chapters/chapter5/figures/rq1/trad-vs-rapid-entire.pdf}
		\label{fig:delivery_delay}
	}
	\subfloat[Triaging phase]{
		\centering
		\includegraphics[width=.45\columnwidth,keepaspectratio]
		{chapters/chapter5/figures/rq1/traditional_vs_rapid_triaging.pdf}
		\label{fig:triaging}
	}

	\subfloat[Fixing phase]{
		\centering
		\includegraphics[width=.45\columnwidth,keepaspectratio]
		{chapters/chapter5/figures/rq1/traditional_vs_rapid_fixtime.pdf}
		\label{fig:fixtime}
	}
	\subfloat[Integration phase]{
		\centering
		\includegraphics[width=.45\columnwidth,keepaspectratio]
		{chapters/chapter5/figures/rq1/traditional_vs_rapid.pdf}
		\label{fig:traditional_vs_rapid}
	}
	\caption{Time spans of the phases involved in the lifetime of an issue.}
	\label{fig:timespans}
\end{figure}

\noindent\DIFdelbegin \textit{\textbf{\DIFdel{Observation~1---There is no significant difference between traditional
and rapid releases regarding issue lifetime.}}%DIFAUXCMD
}%DIFAUXCMD
%DIFDELCMD < \observation{obs:1}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \finding{There is no significant difference between traditional and
rapid releases regarding issue lifetime.}{find17}
\DIFaddend \hyperref[fig:delivery_delay]{Figure}~\ref{fig:delivery_delay} shows the distributions of the lifetime of the
issues in traditional and rapid releases. We observe a $p<1.03e^{-14}$ but a
$negligible$ difference between the distributions ($\textit{delta}=0.03$). We
also observe that traditional releases have a greater MAD ($154$ days) than
rapid releases ($29$ days), which indicates that rapid releases are more
consistent with respect to the lifetime of the issues. Our results indicate that
the difference in the issues' lifetime between traditional and rapid releases is
not as obvious as one might expect. We then look at the triaging, fixing, and
integration time spans to better understand the differences between traditional
and rapid releases.\\

\noindent\DIFdelbegin \textit{\textbf{\DIFdel{Observation~2---Addressed issues are triaged and fixed more quickly in
rapid releases, but tend to wait for a longer time before being
delivered.}}%DIFAUXCMD
}%DIFAUXCMD
%DIFDELCMD < \observation{obs:2}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \finding{Addressed issues are triaged and fixed more quickly in rapid
releases, but tend to wait for a longer time before being delivered.}{find18}
\DIFaddend \hyperref[fig:triaging]{Figures}~\ref{fig:triaging}~,~\ref{fig:fixtime},~and~\ref{fig:traditional_vs_rapid}
show the triaging, fixing, and integration time spans, respectively. We observe
that addressed issues take a median time of 54 days to be integrated into
traditional releases, while taking 104 days (median) to be integrated into rapid
releases. We observe a $p<2.2e^{-16}$ with a $small$ effect-size ($delta=-0.25$).

Regarding fixing time span, an issue takes 6 days (median) to be fixed in
rapid releases, and 9 days (median) in traditional releases. These results
are statistically significant $p<2.2e^{-16}$, but there is only a $negligible$
difference between distributions ($delta=0.13$). 

Our results complement previous research. Khomh~\etal \cite{khomh2012faster}
found that post- and pre-release bugs that are associated with crash reports are
fixed faster in rapid Firefox releases than in traditional releases.
Furthermore, we observe a significant $p<2.2e^{-16}$ but $negligible$
difference ($\textit{delta}=0.11$) between traditional and rapid releases
regarding triaging time. The median triaging time for rapid and traditional
releases are 11 and 18 days, respectively.

When we consider both pre-integration phases together (triaging $t1$ plus
fixing $t2$ in \hyperref[fig:issue_lifecycle]{Figure}~\ref{fig:issue_lifecycle}), we observe that an issue takes
$11$ days (median) to triage and address in rapid releases, while
it takes $19$ days (median) in traditional releases. We observe a $p<2.2e^{-16}$ with a $small$
effect-size ($delta=0.15$). Our results suggest that even though issues have
shorter pre-integration phases in rapid releases, they remain ``on the
shelf'' for a longer time on average.

Finally, we again observe that rapid releases are more consistent than
traditional releases in terms of fixing and \DIFdelbegin \DIFdel{integration }\DIFdelend \DIFaddbegin \DIFadd{delivery }\DIFaddend rate. Rapid releases
achieve MADs of 9 and 17 days for fixing and \DIFdelbegin \DIFdel{integration}\DIFdelend \DIFaddbegin \DIFadd{delivery}\DIFaddend , respectively. The
values for traditional releases are 13 and 64 days for fixing and \DIFdelbegin \DIFdel{integration}\DIFdelend \DIFaddbegin \DIFadd{delivery}\DIFaddend ,
respectively.\\ 

\conclusionbox{Although issues are triaged and fixed faster in rapid releases,
they tend to take a longer time to be integrated. However, the delivery rate
of addressed issues is more consistent in rapid releases than in traditional
ones.}

\subsection{RQ2: Why can traditional releases deliver addressed issues
more quickly?}\label{ch5:rq2} 

\subsubsection*{RQ2: Motivation} 

In \hyperref[ch5:rq1]{RQ1}, we surprisingly find that traditional releases tend
to deliver addressed issues more quickly than rapid releases. This result raises
the following question: why can a traditional release strategy, which has a
longer release cycle, deliver addressed issues more quickly than a rapid
release~strategy?\\

\subsubsection*{RQ2: Approach}

In \hyperref[ch5:rq2]{RQ2}, we group traditional and rapid releases into major
and minor releases and study their delivery delays. As in
\hyperref[ch5:rq1]{RQ1}, we also use beanplots~\cite{kampstra2008beanplot}, MWW
tests~\cite{wilks2011statistical}, and Cliff's delta effect-size
measures~\cite{cliff1993dominance} to perform our comparisons.

\subsubsection*{RQ2: Results}

\begin{figure}[t!]
	\centering
	\includegraphics[width=\textwidth,keepaspectratio]
	{chapters/chapter5/figures/rq2/major_vs_minor.pdf}
	\caption{
		Distributions of delivery delay of addressed issues grouped by minor and major
		releases.
	}
	\label{fig:major_vs_minor}
\end{figure}

\begin{sloppypar}
\noindent\DIFdelbegin \textit{\textbf{\DIFdel{Observation~3---Minor-traditional releases tend to have
less delivery delay than major/minor-rapid releases.}}%DIFAUXCMD
}%DIFAUXCMD
%DIFDELCMD < \observation{obs:3}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \finding{Minor-traditional releases tend to have
less delivery delay than major/minor-rapid releases.}{find19}
\DIFaddend \hyperref[fig:major_vs_minor]{Figure}~\ref{fig:major_vs_minor} shows the distributions of delivery delay
grouped by (1) \textit{major-traditional vs. minor-traditional}, (2)
\textit{major-traditional vs. rapid}, (3) \textit{major-rapid vs. minor-rapid},
and (4) \textit{minor-traditional vs. minor-rapid}. In the comparison of
\textit{major-traditional vs.  minor-traditional}, we observe that
minor-traditional releases are mainly associated with shorter delivery delay.
Furthermore, in the comparison \textit{major-traditional vs. rapid}, rapid
releases deliver addressed issues more quickly than major-traditional releases
on average ($p<2.2e^{-16}$ with a $medium$ effect-size, \ie  $delta=0.40$). 
\end{sloppypar}

The Firefox rapid release cycle includes ESR releases (see
\hyperref[ch:background]{Chapter}~\ref{ch:background}) and a few minor
stabilization and security releases. These releases also deliver addressed
issues more quickly than major-rapid releases (\textit{major-rapid vs.
minor-rapid}) with a $p<2.2e^{-16}$ and a $large$ effect-size, \ie $delta=0.92$.
Furthermore, we do not observe a statistically significant difference between
distributions in the comparison of \textit{minor-traditional vs. minor-rapid}
($p=0.68$).

Minor-traditional releases have the lowest delivery delay (median of 25
days). This is likely because they are more focused on a particular set of
issues that, once addressed, should be released immediately. For example, the
release history documentation of Firefox shows that minor releases are usually
related to stability and security
issues.\DIFdelbegin \footnote{%DIFDELCMD < \url{https://www.mozilla.org/en-US/firefox/releases/}%%%
}%DIFAUXCMD
\addtocounter{footnote}{-1}%DIFAUXCMD
\DIFdelend \DIFaddbegin \smartfoot{\url{https://www.mozilla.org/en-US/firefox/releases/}}\DIFaddend \\

\begin{figure}[t!]
	\centering
	\subfloat[Only Major]{           
		\includegraphics[width=0.45\columnwidth,keepaspectratio]
		{chapters/chapter5/figures/releases/majorreleases_length.pdf}
		\label{fig:majorreleases_length}
	}
	\subfloat[Major and Minor]{
		\includegraphics[width=0.45\columnwidth,keepaspectratio]
		{chapters/chapter5/figures/releases/releases_length.pdf}
		\label{fig:releases_length}
	}
	\caption{Release frequency (in days). The outliers in figure~(b)
		represent the major-traditional releases.}
	\label{fig:release_length_analysis}
\end{figure}

\noindent\DIFdelbegin \textit{\textbf{\DIFdel{Observation~4---When considering both
minor and major releases, the time span between traditional and rapid releases
are roughly the same.}}%DIFAUXCMD
}%DIFAUXCMD
%DIFDELCMD < \observation{obs:4}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \finding{When considering both minor and major releases, the time span
between traditional and rapid releases are roughly the same.}{find20} \DIFaddend Since we
observe that delivery delay is shorter on average in traditional releases, we
also investigate the length of the release cycles to better understand our
previous results (see \DIFdelbegin %DIFDELCMD < \hyperref[obs:2]{Observation}%%%
\DIFdel{~\ref{obs:2}}\DIFdelend \DIFaddbegin \hyperref[find19]{Finding}\DIFadd{~\ref{find19}}\DIFaddend ).
\hyperref[fig:majorreleases_length]{Figure}~\ref{fig:majorreleases_length} shows
that, at first glance, one may speculate that rapid releases should deliver
addressed issues more quickly because releases are produced more frequently.
However, if we consider both major and minor releases---as shown in
\hyperref[fig:releases_length]{Figure}~\ref{fig:releases_length}---we observe
that both release strategies deliver releases at roughly the same rate on
average (median of $40$ and $42$ days for traditional and rapid releases,
respectively).\\

\conclusionbox{
Minor-traditional releases are one of the main reasons why the traditional
release strategy can deliver addressed issues more quickly than the rapid
release strategy. Furthermore, the lengths of the release cycles are roughly the
same between traditional and rapid releases when both minor and major releases
are considered.
}

\subsection{RQ3: Did the change in the release strategy have an impact on
the characteristics of delayed issues?}\label{ch5:rq3} 

\subsubsection*{RQ3: Motivation}

In \hyperref[ch5:rq1]{RQ1} and \hyperref[ch5:rq2]{RQ2}, we study the differences
between rapid and traditional releases with respect to delivery delay. We find
that although issues tend to be addressed more quickly in rapid releases, they
tend to wait longer to be delivered. We also find that the use of minor releases
is a key reason as to why traditional releases may deliver addressed issues more
quickly. In \hyperref[ch4:rq3]{RQ3}, we investigate what are the characteristics
of each release strategy that are associated with delivery delays. This
important investigation sheds light on what may generate delivery delays in each
release strategy, so that projects are aware of the characteristics of rapid
releases versus traditional releases before choosing to adopt one of these
release strategies. 

\subsubsection*{RQ3: Approach}

\begin{table}[!t]
	\footnotesize
	\caption{Metrics that are used in our explanatory models (Reporter, Resolver,
		and Issue families).
	\label{tbl:factors1}}
	\centering
		\begin{tabular}{rp{2cm}lp{9cm}}
			\hline
			\multicolumn{1}{c}{\textbf{Family}} &
			\multicolumn{1}{c}{\textbf{Metrics}} & \multicolumn{1}{c}{\textbf{Value}} &
			\multicolumn{1}{c}{\textbf{Definition (d)$\vert$Rationale (r)}}
			\\ \hline
			\multicolumn{ 1}{r}{\textbf{Reporter}} & Experience & Numeric & 
			\begin{tabular}{p{8.7cm}}
				\textbf{d:} the number of previously delivered issues that were reported by
				the reporter of a particular addressed issue.  \\ \hline 
				\textbf{r:} The greater the experience of the reporter the
				higher the quality of his/her reports and the
				solution to his/her reports might be delivered
				more quickly~\cite{shihab2010predicting}.
			\end{tabular}
			\\ \cline{2- 4}
			\multicolumn{ 1}{r}{} & Reporter integration & Numeric &
			\begin{tabular}{p{8.7cm}}
				\textbf{d:} The median in days of the previously delivered addressed issues
				that were reported by a particular reporter.
				\\ \hline 
				\textbf{r:} If a particular reporter usually reports issues that are
				delivered quickly, his/her future reported issues might be delivered
				quickly as well.
			\end{tabular}
			\\ \hline
			\multicolumn{ 1}{r}{\textbf{Resolver}} & Experience & Numeric & 
			\begin{tabular}{p{8.7cm}}
				\textbf{d:} the number of previously delivered addressed issues that were
				addressed by the resolver of a particular
				addressed issue. We consider the collaborator
				that changed the status of an issue to
				RESOLVED-FIXED as the resolver of that issue. \\ \hline 
				\textbf{r:} The greater the experience of the resolver, the
				greater the likelihood that his/her code will be
				delivered faster~\cite{shihab2010predicting}.
			\end{tabular}
			\\ \cline{2- 4}
			\multicolumn{ 1}{r}{} & Resolver integration & Numeric &
			\begin{tabular}{p{8.7cm}}
				\textbf{d:} The median in days of the previously delivered addressed issues
				that were addressed by a particular resolver.
				\\ \hline 
				\textbf{r:} If a particular resolver usually address issues that are
				delivered quickly, his/her future addressed issues might be delivered
				quickly as well.
			\end{tabular}
			\\ \hline
			\multicolumn{ 1}{r}{\textbf{Issue}} & Stack trace attached & Boolean &
			\begin{tabular}{p{8.7cm}}
				\textbf{d:} We verify if the issue report has a stack trace attached in its description. \\ \hline 
				\textbf{r:} A stack trace attached may provide useful information regarding
				the cause of the issue, which may quicken the
				\DIFdelbeginFL \DIFdelFL{integration }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{delivery }\DIFaddendFL of the addressed issue~\cite{schroter2010stack}. 
			\end{tabular}
			\\ \cline{2- 4}
			\multicolumn{ 1}{r}{} & Severity & Nominal &
			\begin{tabular}{p{8.7cm}}
				\textbf{d:} The severity level of the issue report. Issues with higher
				severity levels (\eg blocking) might be delivered faster than other issues.
				\\ \hline 
				\textbf{r:} Panjer observed that the severity of an issue has a large effect
				on its time to be addressed in the Eclipse project~\cite{Panjer2007}.
			\end{tabular}
			\\ \cline{2- 4}
			\multicolumn{ 1}{r}{} & Priority & Nominal &
			\begin{tabular}{p{8.7cm}}
				\textbf{d:} The priority level of the issue report. Issues with higher
				priority levels (\eg P1) might be delivered faster than other issues.
				\\ \hline 
				\textbf{r:} Higher priority issues will likely be delivered before lower
				priority issues.
			\end{tabular}
			\\ \cline{2- 4}
			\multicolumn{ 1}{r}{} & Description size & Numeric &
			\begin{tabular}{p{5.6cm}}
				\textbf{d:} The number of words in the description of the issue. 
				\\ \hline 
				\textbf{r:} Issues that are well described might be more easy to integrate
				than issues that are difficult to understand.
			\end{tabular}
			\\ \hline
		\end{tabular}
\end{table}

\begin{table}[!t]
	\footnotesize
	\caption{Metrics that are used in our explanatory models (Project family).
	\label{tbl:factors2}}
	\centering
		\begin{tabular}{rp{2cm}lp{9cm}}
			\hline
			\multicolumn{1}{c}{\textbf{Family}} &
			\multicolumn{1}{c}{\textbf{Metrics}} & \multicolumn{1}{c}{\textbf{Value}} &
			\multicolumn{1}{c}{\textbf{Definition (d)$\vert$Rationale (r)}}
			\\ \hline
			\multicolumn{ 1}{r}{\textbf{Project}} & Queue rank & Numeric &
			\begin{tabular}{p{8.7cm}}
				\textbf{d:} A rank number that represents the
				moment at which an issue is
				addressed compared to other addressed issues in the backlog. For instance,
				in a backlog that contains $500$ issues, the first addressed issue has
				a rank of~1, while the last addressed issue has
				a rank of 500.
				\\ \hline 
				\textbf{r:} An issue with a high \textit{queue rank} is a recently
				addressed issue. An addressed issue might be delivered faster/slower
				depending of its rank.
			\end{tabular}
			\\ \cline{2- 4}
			\multicolumn{ 1}{r}{} & Cycle queue rank & Numeric &
			\begin{tabular}{p{8.7cm}}
				\textbf{d:} A rank number that represents the
				moment at which an issue is
				addressed compared to other addressed issues of the same release cycle. For
				example, in a release cycle that contains $300$ addressed issues, the first
				addressed issue has a rank of $1$, while the
				last one has a rank of $300$.
				\\ \hline 
				\textbf{r:} An issue with a high \textit{cycle queue rank} is a recently
				addressed issue compared to the others of the same release cycle. An issue
				addressed close to the upcoming release might be delivered faster.
			\end{tabular}
			\\ \cline{2- 4}
			\multicolumn{ 1}{r}{} & Queue position & Numeric &
			\begin{tabular}{p{8.7cm}}
				\textbf{d:} $\frac{\text{queue rank}}{\text{all addressed issues}}$. The
				\textit{queue rank} is divided by all the issues that are addressed by the
				end of the next release. A \textit{queue position} close to 1 indicates that the
				issue was addressed recently compared to others in the backlog.
				\\ \hline 
				\textbf{r:} An addressed issue
				might be delivered faster/slower depending of its position.
			\end{tabular}
			\\ \cline{2- 4}
			\multicolumn{ 1}{r}{} & Cycle queue position & Numeric &
			\begin{tabular}{p{8.7cm}}
				\textbf{d:} $\frac{\text{cycle queue rank}}{\text{addressed issues of the
				current cycle}}$. The \textit{cycle queue rank} is divided by all of the
				addressed issues of the release cycle. A \textit{cycle
				queue position} close to 1 indicates that the issue was addressed recently
				in the release cycle.
				\\ \hline 
				\textbf{r:}  An issue addressed close
				to a upcoming release might be delivered faster. 
			\end{tabular}
			\\ \hline
		\end{tabular}
\end{table}

\begin{table}[!t]
	\footnotesize
	\caption{Metrics that are used in our explanatory models (Process family).
	\label{tbl:factors3}}
	\centering
		\begin{tabular}{rp{2cm}lp{9cm}}
			\hline
			\multicolumn{1}{c}{\textbf{Family}} &
			\multicolumn{1}{c}{\textbf{Metrics}} & \multicolumn{1}{c}{\textbf{Value}} &
			\multicolumn{1}{c}{\textbf{Definition (d)$\vert$Rationale (r)}}
			\\ \hline
			\multicolumn{ 1}{r}{\textbf{Process}} & Number of Impacted Files & Numeric & 
			\begin{tabular}{p{8.7cm}}
				\textbf{d:} The number of files that are linked to an issue report. \\ \hline 
				\textbf{r:} A delivery delay might be
				related to a high number of impacted files 
				because more effort would be required to properly integrate the modifications 
				\cite{Jiang2013}. 
			\end{tabular}
			\\ \cline{ 2- 4}
			\multicolumn{ 1}{r}{} & Churn & Numeric &
			\begin{tabular}{p{8.7cm}}
				\textbf{d:} The sum of added lines plus the sum
				of deleted lines to address the issue.\\ \hline 
				\textbf{r:} A higher churn suggests that a great amount of work was required
				to address the issue, and hence, verifying the impact of integrating the
				modifications may also be difficult~\cite{Jiang2013,Nagappan2005}.
			\end{tabular}
			\\ \cline{2- 4}
			\multicolumn{ 1}{r}{} & Fix time & Numeric &
			\begin{tabular}{p{8.7cm}}
				\textbf{d:} Number of days between the date when
				the issue was
				triaged and the
				date that it was addressed~\cite{Giger2010}. \\ \hline 
				\textbf{r:} If an issue is addressed quickly, it may have a better chance to
				be delivered faster.
			\end{tabular}
			\\ \cline{ 2- 4}
			\multicolumn{ 1}{r}{} & Number of activities & Numeric &
			\begin{tabular}{p{8.7cm}}
				\textbf{d:} An activity is an entry in the issue's history. \\ \hline 
				\textbf{r:} A high number of activities might indicate that much work was
				required to address the issue, which may impact
				the integration \DIFaddbeginFL \DIFaddFL{and delivery }\DIFaddendFL of the
				issue\DIFdelbeginFL \DIFdelFL{into a release}\DIFdelendFL ~\cite{Jiang2013}.
			\end{tabular}
			\\ \cline{ 2- 4}
			\multicolumn{ 1}{r}{} & Number of comments & Numeric & 
			\begin{tabular}{p{8.7cm}}
				\textbf{d:} The number of comments of an issue report. \\ \hline 
				\textbf{r:} A large number of comments might indicate the importance of an
				issue or the difficulty to understand it~\cite{Giger2010}, which might
				impact the delivery delay~\cite{Jiang2013}.
			\end{tabular}
			\\ \cline{ 2- 4}
			\multicolumn{ 1}{r}{} & Interval of comments & Numeric &
			\begin{tabular}{p{8.7cm}}
				\textbf{d:} The sum of the time intervals (hour) between
				comments divided by the total number of comments of an issue report. \\ \hline 
				\textbf{r:} A short \textit{interval of comments} indicates that an intense
				discussion took place, which suggests that the issue is important. Hence,
				such an issue may be delivered faster.   
			\end{tabular}
			\\ \cline{ 2- 4}
			\multicolumn{ 1}{r}{} & Number of tosses & Numeric &
			\begin{tabular}{p{8.7cm}}
				\textbf{d:} The number of times that the assignee has changed. \\ \hline 
				\textbf{r:} Changes in the issue assignee might indicate that more than one
				developer have worked on the issue. Such issues may be more difficult to
				integrate, since different expertise from different developers might 
				be required~\cite{Jeong2009,Jiang2013}.  
			\end{tabular}
			\\ \hline
		\end{tabular}
\end{table}

For \hyperref[ch5:rq3]{RQ3}, we build explanatory models (\ie logistic
regression models) for the traditional and rapid releases data using the metrics
that are presented in
\hyperref[tbl:factors1]{Tables}~\ref{tbl:factors1},~\ref{tbl:factors2}, and
\ref{tbl:factors3}. We model our response variable $Y$ as $Y=1$ for addressed
issues that are delayed, \ie had their delivery prevented in at least one
release\cite{costa2014empirical} and $Y=0$ otherwise. Hence, our models are
intended to explain why a given addressed issue has a delayed delivery (\ie
$Y=1$).

\begin{figure}[t!]
	\centering
	\includegraphics[width=\columnwidth,keepaspectratio]
	{chapters/chapter5/figures/rq3/model_construction.pdf}
	\caption{Overview of the process that we use to build our explanatory models.}
	\label{fig:model_construction}
\end{figure}

We follow the guidelines of Harrell~Jr.~\cite{harrell2001regression} for
building explanatory regression models.
\hyperref[fig:model_construction]{Figure}~\ref{fig:model_construction} provides
an overview of the process that we use to build our models. First, we estimate
the budget of degrees of freedom that we can spend on our models while having a
low risk of overfitting (\ie producing a model that is too specific to the
training data to be useful when applied to other unseen data). Second, we check for
metrics that are highly correlated using Spearman rank correlation tests
$(\rho)$ and we perform a redundancy analysis to remove any redundant metrics
before building our explanatory models. 

We then assess the fit of our models using the ROC area and the Brier score. The
ROC area is used to evaluate the degree of discrimination that is achieved by
a model. The ROC values range between 0 (worst) and 1 (best). An area greater
than 0.5 indicates that the explanatory model outperforms na\"{i}ve random
guessing models. The Brier score is used to evaluate the accuracy of
probabilistic predictions. This score measures the mean squared difference
between the probability of delay assigned by our models for a particular issue
$I$ and the actual outcome of $I$ (\ie whether $I$ is actually delayed or not).
Hence, the lower the Brier score, the more accurate the probabilities that are
produced by a model.

Next, we assess the stability of our models by computing the
\textit{optimism-reduced} ROC area and Brier score~\cite{efron1986biased}. The
optimism of each metric is computed by selecting a bootstrap sample to
fit a model with the same degrees of freedom of the original model. The model
that is trained using the bootstrap sample is applied both on the bootstrap and original
samples (ROC and Brier scores are computed for each sample). The optimism is the
difference in the ROC area and Brier score of the bootstrap sample and original
sample. This process is repeated 1,000 times and the average optimism is
computed. Finally, we obtain the \textit{optimism-reduced} scores by subtracting
the average optimism from the initial ROC area and Brier score
estimates~\cite{efron1986biased}.

We evaluate the impact of each metric on the fitted models using
Wald $\chi^2$ maximum likelihood tests. The larger the $\chi^2$ value, the
larger the impact that a particular metric has on our explanatory models'
performance. We also study the relationship that our metrics 
share with the likelihood of delivery delay. To do so, we plot the change in
the estimated probability of delay against the change in a given metric
while holding the other metrics constant at their median values using the
\code{Predict} function of the \code{rms} package~\cite{harrell2001regression}. 

We also plot nomograms~\cite{iasonos2008build,harrell2001regression} to evaluate
the impact of the metrics in our models. Nomograms are user-friendly charts that
visually represent explanatory models. For instance,
\hyperref[fig:nomogram_trad]{Figure}~\ref{fig:nomogram_trad} shows the nomogram
of the model that we fit for the rapid release data. The higher the number of
points that are assigned to an explanatory metric on the $x$ axis (\eg 100
points are assigned to \textit{comments} in rapid releases), the larger the
effect of that metric in the explanatory model. We compare which metrics are
more important in both traditional and rapid releases in order to better
understand the differences between these release strategies.

\subsubsection*{RQ3: Results}

\begin{sloppypar}
\noindent\DIFdelbegin \textit{\textbf{\DIFdel{Observation~5---Our models achieve a Brier score of 0.05-0.16 and ROC
areas of 0.81-0.83.}}%DIFAUXCMD
}%DIFAUXCMD
%DIFDELCMD < \observation{obs:5}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \finding{Our models achieve a Brier score of 0.05-0.16 and ROC
areas of 0.81-0.83.}{find21}
\DIFaddend The models that we fit to traditional releases achieve a Brier score of 0.16 and
an ROC area of 0.83, while the models that we fit to the rapid release data
achieve a Brier score of 0.05 and an ROC area of 0.81. Our models outperform
na\"{i}ve approaches such as random guessing and ZeroR---our ZeroR models
achieve ROC areas of 0.5 and Brier scores of 0.06 and 0.45 for rapid and
traditional releases, respectively. Moreover, the bootstrap-calculated optimism
is less than 0.01 for both the ROC areas and Brier scores of our models. This
result shows that our regression models are stable enough to perform the
statistical inferences that follow.\\

\begin{table}[t]
	\scriptsize
	\begin{center}
		\caption{Overview of the regression model fits. The $\chi^2$ of
		each metric is shown as the proportion in relation to the total
	$\chi^2$ of the model.  \label{tbl:regression_models} }
		%\resizebox{\textwidth}{!}{
			\begin{tabular}{cccc}
			\cline{3-4} 
			\multicolumn{2}{c}{} & 
			Traditional releases &
			Rapid releases 
			\tabularnewline
			\hline 
			\multicolumn{2}{c}{\# of instances} & 
			$34,673$ &
			$37,441$  
			\tabularnewline
			\hline
			\multicolumn{2}{c}{Wald $\chi^2$} & 
			$4,964$ &
			$2,705$  
			\tabularnewline
			\hline 
			\multicolumn{2}{c}{Budgeted Degrees of Freedom} &
			$1033$ & 
			$149$ 
			\tabularnewline
			\hline
			\multicolumn{2}{c}{Degrees of Freedom Spent} &
			$26$ & 
			$25$ 
			\tabularnewline
			%\hline 
			%\hline 
			%&  &  &  
			%\tabularnewline
			\hline 
			\multirow{2}{*}{Reporter experience} & 
			D.F. & 
			$1$ & 
			$1$  
			\tabularnewline 
			& 
			$\chi^2$ & 
			$2^{\ast\ast\ast}$ &  
			$2^{\ast\ast\ast}$ 
			\tabularnewline
			\hline 
			\multirow{2}{*}{Reporter integration} & 
			D.F. & 
			$1$ & 
			$1$  
			\tabularnewline 
			& 
			$\chi^2$ & 
			$5^{\ast\ast\ast}$ &  
			$4^{\ast\ast\ast}$ 
			\tabularnewline
			\hline 
			\multirow{2}{*}{Resolver Experience} & 
			D.F. & 
			$1$ & 
			\multirow{2}{*}{$\oslash$}
			\tabularnewline &
			$\chi^2$ & 
			$1^{\ast\ast\ast}$ &
			\tabularnewline 
			\hline 
			\multirow{2}{*}{Resolver integration} & 
			D.F. & 
			$1$ & 
			$1$  
			\tabularnewline 
			& 
			$\chi^2$ & 
			$2^{\ast\ast\ast}$ &  
			$5^{\ast\ast\ast}$ 
			\tabularnewline
			\hline 
			\multirow{2}{*}{Fix time} & 
			D.F. & 
			1 &
			1  
			\tabularnewline & 
			$\chi^2$ &
			$2^{\ast\ast\ast}$ &
			$8^{\ast\ast\ast}$ 
			\tabularnewline \hline 
			\multirow{2}{*}{Severity} &
			D.F. & 
			$6$ & 
			$6$ 
			\tabularnewline & 
			$\chi^2$ & 
			$1^{\ast\ast\ast}$ &  
			$1^{\ast\ast\ast}$  
			\tabularnewline \hline 
			\multirow{2}{*}{Priority} & 
			D.F. & 
			$5$ & 
			$5$ 
			\tabularnewline & 
			$\chi^2$ & 
			$1^{\ast\ast\ast}$ &  
			$\approx 0$   
			\tabularnewline \hline 
			\multirow{2}{*}{Size of description} & 
			D.F. & 
			$1$ &
			$1$  
			\tabularnewline & 
			$\chi^2$ & 
			$\approx 0$ &
			$1^{\ast\ast\ast}$   
			\tabularnewline \hline 
			\multirow{2}{*}{Stack trace attached} & 
			D.F. & 
			$1$ &
			$1$  
			\tabularnewline & 
			$\chi^2$ & 
			$\approx 0$ &  
			$\approx 0$ 
			\tabularnewline \hline 
			\multirow{2}{*}{Number of files} & 
			D.F. & 
			$1$ & 
			$1$
			\tabularnewline & 
			$\chi^2$ & 
			$1^{\ast\ast\ast}$ &  
			$1^{\ast\ast\ast}$ 
			\tabularnewline \hline 
			\multirow{2}{*}{Number of comments} & 
			D.F. & 
			$1$ &
			$1$ 
			\tabularnewline & 
			$\chi^2$ & 
			$\approx 0^{\ast}$ &  
			$31^{\ast\ast\ast}$  
			\tabularnewline \hline 
			\multirow{2}{*}{Number of tossing} & 
			D.F. & 
			$1$ &
			$1$  
			\tabularnewline & 
			$\chi^2$ & 
			$\approx 0^{\ast\ast\ast}$ &  
			$\approx 0$   
			\tabularnewline \hline 
			\multirow{2}{*}{Number of activities} & 
			D.F. & 
			$1$ &
			$1$  
			\tabularnewline & 
			$\chi^2$ & 
			$1^{\ast\ast\ast}$ &  
			$3^{\ast\ast\ast}$  
			\tabularnewline \hline 
			\multirow{2}{*}{Interval of comments} & 
			D.F. & 
			\multirow{2}{*}{$\oslash$} &
			\multirow{2}{*}{$\oslash$}  
			\tabularnewline &
			$\chi^2$ &
			&
			\tabularnewline \hline 
			\multirow{2}{*}{Code churn} & 
			D.F. & 
			$1$ & 
			$1$  
			\tabularnewline &
			$\chi^2$ &
			$\approx 0$ &  
			$\approx 0$ 
			\tabularnewline \hline 
			\multirow{2}{*}{Queue position} & 
			D.F. & 
			$1$ &             
			$1$
			\tabularnewline & 
			$\chi^2$ & 
			$17^{\ast\ast\ast}$ & 
			$2^{\ast\ast\ast}$
			\tabularnewline \hline 
			\multirow{1}{*}{Queue rank} & 
			D.F. & 
			$1$ & 
			$1$ 
			\tabularnewline &
			$\chi^2$ & 
			$56^{\ast\ast\ast}$ & 
			$14^{\ast\ast\ast}$
			\tabularnewline \hline 
			\multirow{2}{*}{Cycle queue rank} & 
			D.F. & 
			$1$ &
			$1$ 
			\tabularnewline &
			$\chi^2$ & 
			$10^{\ast\ast\ast}$ &
			$28^{\ast\ast\ast}$ 
			\tabularnewline \hline 
			\multirow{2}{*}{Cycle queue position} & 
			D.F. & 
			\multirow{2}{*}{$\oplus$} &
			\multirow{2}{*}{$\oslash$}  
			\tabularnewline &
			$\chi^2$ & 
			&
			\tabularnewline \hline 
		\end{tabular}
%	}
	\captionsetup{justification=centering}
	\caption*{
		$\oslash$ discarded during correlation analysis \\
		$\oplus$ discarded during redundancy analysis \\
		$\ast$ $p < 0.05$;
		$\ast\ast$ $p < 0.01$;
		$\ast\ast\ast$ $p < 0.001$\\ 
	}
	\end{center}
\end{table}

\noindent\DIFdelbegin \textit{\textbf{\DIFdel{Observation~6---Traditional releases prioritize the
delivery of backlog issues, while rapid releases prioritize the delivery
of issues of the current release cycle.}}%DIFAUXCMD
}%DIFAUXCMD
%DIFDELCMD < \observation{obs:6}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \finding{Traditional releases prioritize the delivery
of backlog issues, while rapid releases prioritize the delivery of issues of the
current release cycle.}{find22}
\DIFaddend \hyperref[tbl:regression_models]{Table}~\ref{tbl:regression_models} shows the
explanatory power ($\chi^2$) of each metric that we use in our models.  The
\textit{queue rank} metric is the most important metric in the models that we
fit to the traditional release data.  Queue rank measures the moment when an
issue is addressed in the backlog of the project (see
\hyperref[tbl:factors2]{Table}~\ref{tbl:factors2}).
\hyperref[fig:rankposition]{Figure}~\ref{fig:rankposition} shows the
relationship that queue rank shares with delivery delay. Our models reveal that
the addressed issues in traditional releases have a higher likelihood of being
delayed if they are addressed later when compared to other issues in the backlog
of the project.   

\begin{figure}[t]
	\centering
	\subfloat{
		\includegraphics[width=0.43\textwidth,keepaspectratio]
		{chapters/chapter5/figures/rq3/queue_rank.pdf}
		\label{fig:rankposition}
	}
	\subfloat{
		\includegraphics[width=0.43\textwidth,keepaspectratio]
		{chapters/chapter5/figures/rq3/cycle_queue_rank.pdf}
		\label{fig:cycle_rank}
	}

	\subfloat{
		\includegraphics[width=0.43\textwidth,keepaspectratio]
		{chapters/chapter5/figures/rq3/comments.pdf}
		\label{fig:number_comments}
	}
	\caption{The relationship between metrics and delivery delay. The blue
		line shows the values of our model fit, whereas the grey
		area shows the 95\% confidence interval based on models fit to
		1,000 bootstrap samples. The parentheses indicate the
		release strategy to which the metric is related.
	}
\end{figure}

\begin{figure}[!]
	\centering
	\includegraphics[width=0.90\textwidth,keepaspectratio]
	{chapters/chapter5/figures/rq3/nomogram_trad.pdf}
	\caption{Nomogram of our explanatory models for the traditional release cycle.}
	\label{fig:nomogram_trad}
\end{figure}

\begin{figure}[!]
	\includegraphics[width=0.90\textwidth,keepaspectratio]
	{chapters/chapter5/figures/rq3/nomogram_rapid.pdf}
	\caption{Nomogram of our explanatory models for the rapid release cycle.}
	\label{fig:nomogram_rapid}
\end{figure}

On the other hand, \textit{cycle queue rank} is the second-most important metric
in the models that we fit to the rapid release data. Cycle queue rank is the
moment when an issue is addressed in a given release cycle.
\hyperref[fig:cycle_rank]{Figure}~\ref{fig:cycle_rank} shows the relationship that cycle queue rank shares
with delivery delay. Our models reveal that the addressed issues in rapid
releases have a higher likelihood of being delayed if they were addressed later
than other addressed issues in the \textit{current release cycle}.
Interestingly, we observe that the most important metric in our rapid release
models is the \textit{number of comments}.
\hyperref[fig:number_comments]{Figure}~\ref{fig:number_comments}
shows the relationship that the \textit{number of comments} shares with
delivery delay. We observe that the greater the number of comments of an
addressed issue, the greater the likelihood of delivery delay. This result
corroborates the intuition that a lengthy discussion might be indicative of a
complex issue, which may be more likely to be delayed.

Moreover,
\hyperref[fig:nomogram_trad]{Figures}~\ref{fig:nomogram_trad}~and~\ref{fig:nomogram_rapid}
show the estimated effect of our metrics using
nomograms~\cite{iasonos2008build}. Indeed, our nomograms reiterate the large
impact of \textit{number of comments} (100 points) and \textit{cycle queue rank}
(84 points) in rapid releases, and the large impact of \textit{queue rank} (100
points) in traditional releases.  We also observe that \textit{stack trace
attached} has a large impact on traditional releases (68 points) despite not
being a significant contributor to the fit of our models (\cf
\hyperref[tbl:regression_models]{Table}~\ref{tbl:regression_models}). The large
impact shown in our nomogram for \textit{stack trace attached} is due to the
skewness of our data---only $5$ instances within the traditional release data
have the \textit{stack trace attached} set to true. Thus, \textit{stack trace
attached} cannot significantly contribute to the overall fit~of~our~models.

Another \textit{key} difference between traditional
and rapid releases is how addressed issues are prioritized for delivery.
Traditional releases are analogous to a queue in which the earlier an issue is
addressed, the lower its likelihood of delay. On the other hand, rapid
releases are analogous to a stack of cycles, in which the earlier an issue is
addressed in the current cycle, the lower its likelihood of delay.\\
\end{sloppypar}

\conclusionbox{Issues that are addressed early in the
	project backlog are less likely to be delayed in traditional releases.
	On the other hand, issues in rapid releases are queued up on a per
	release basis, in which issues that are addressed early in the release
cycle of the current release are less likely to be delayed.}
%DIF < \subsubsection*{Research Approach of Study 3} \label{ch5:approach_study3}
%DIF < 
%DIF < \begin{figure}[t!]
%DIF < 	\centering
%DIF < 	\includegraphics[width=\columnwidth,keepaspectratio]
%DIF < 	{chapters/chapter5/figures/rq1/issue_lifecycle.pdf}
%DIF < 	\caption{A simplified life cycle of an issue.}
%DIF < 	\label{fig:issue_lifecycle}
%DIF < \end{figure}
%DIF < 
%DIF < \hyperref[fig:issue_lifecycle]{Figure}~\ref{fig:issue_lifecycle} shows a
%DIF < simplified life cycle of an issue, which includes the triaging phase ({\em t1}),
%DIF < the fixing phase ({\em t2}), and the integration phase ({\em t3}). We consider
%DIF < the last RESOLVED-FIXED status as the moment at which a particular issue was
%DIF < addressed (the fixed state in
%DIF < \hyperref[fig:issue_lifecycle]{Figure}~\ref{fig:issue_lifecycle}). The
%DIF < \textit{lifetime} of an issue is composed of all three phases (from \textit{new}
%DIF < to \textit{released}). For \hyperref[ch5:rq1]{RQ1}, we first observe the lifetime of the issues of
%DIF < traditional and rapid releases.  Next, we look at the time span of the
%DIF < \textit{triaging}, \textit{fixing}, and \textit{integration} phases within the
%DIF < lifetime of an issue. In \hyperref[ch5:rq2]{RQ2}, we group traditional and rapid releases into major
%DIF < and minor releases and study their delivery delays.
%DIF < 
%DIF < We use beanplots~\cite{kampstra2008beanplot} to compare the distributions of our
%DIF < data. The vertical curves of beanplots summarize and compare the distributions
%DIF < of different datasets (see
%DIF < \hyperref[fig:delivery_delay]{Figure}~\ref{fig:delivery_delay}). The higher
%DIF < the frequency of data within a particular value, the thicker the bean is plotted
%DIF < at that particular value on the $y$ axis. We also use Mann-Whitney-Wilcoxon
%DIF < (MWW) tests~\cite{wilks2011statistical} and Cliff's delta effect-size
%DIF < measures~\cite{cliff1993dominance}. MWW tests are non-parametric tests of the
%DIF < \textit{null hypothesis} that two distributions come from the same population
%DIF < ($\alpha=0.05$). On the other hand, Cliff's delta is a non-parametric
%DIF < effect-size measure to verify the difference in magnitude of one distribution compared
%DIF < to another distribution. The higher the value of the Cliff's delta,
%DIF < the greater the difference of values between distributions. For instance, if we
%DIF < obtain a significant $p$ value but a small Cliff's delta, this means that
%DIF < although two distributions do not come from the same population their 
%DIF < difference is not that large. A positive Cliff's delta indicates how much
%DIF < larger the values of the first distribution are, while a negative Cliff's delta
%DIF < indicates the inverse. Finally, we use the \textit{Median Absolute Deviation}
%DIF < (MAD) \cite{howell2005median,leys2013detecting} as a measure of the variation of
%DIF < our distributions. The MAD is the median of the \textit{absolute deviations}
%DIF < from one distribution's median. The higher the MAD, the greater is the variation
%DIF < of a distribution with respect to its median.
%DIF < 
%DIF < \begin{table}[!t]
%DIF < 	\footnotesize
%DIF < 	\caption{Metrics that are used in our explanatory models (Reporter, Resolver,
%DIF < 		and Issue families).
%DIF < 	\label{tbl:factors1}}
%DIF < 	\centering
%DIF < 		\begin{tabular}{rp{2cm}lp{9cm}}
%DIF < 			\hline
%DIF < 			\multicolumn{1}{c}{\textbf{Family}} &
%DIF < 			\multicolumn{1}{c}{\textbf{Metrics}} & \multicolumn{1}{c}{\textbf{Value}} &
%DIF < 			\multicolumn{1}{c}{\textbf{Definition (d)$\vert$Rationale (r)}}
%DIF < 			\\ \hline
%DIF < 			\multicolumn{ 1}{r}{\textbf{Reporter}} & Experience & Numeric & 
%DIF < 			\begin{tabular}{p{8.7cm}}
%DIF < 				\textbf{d:} the number of previously delivered issues that were reported by
%DIF < 				the reporter of a particular addressed issue.  \\ \hline 
%DIF < 				\textbf{r:} The greater the experience of the reporter the
%DIF < 				higher the quality of his/her reports and the
%DIF < 				solution to his/her reports might be delivered
%DIF < 				more quickly~\cite{shihab2010predicting}.
%DIF < 			\end{tabular}
%DIF < 			\\ \cline{2- 4}
%DIF < 			\multicolumn{ 1}{r}{} & Reporter integration & Numeric &
%DIF < 			\begin{tabular}{p{8.7cm}}
%DIF < 				\textbf{d:} The median in days of the previously delivered addressed issues
%DIF < 				that were reported by a particular reporter.
%DIF < 				\\ \hline 
%DIF < 				\textbf{r:} If a particular reporter usually reports issues that are
%DIF < 				delivered quickly, his/her future reported issues might be delivered
%DIF < 				quickly as well.
%DIF < 			\end{tabular}
%DIF < 			\\ \hline
%DIF < 			\multicolumn{ 1}{r}{\textbf{Resolver}} & Experience & Numeric & 
%DIF < 			\begin{tabular}{p{8.7cm}}
%DIF < 				\textbf{d:} the number of previously delivered addressed issues that were
%DIF < 				addressed by the resolver of a particular
%DIF < 				addressed issue. We consider the collaborator
%DIF < 				that changed the status of an issue to
%DIF < 				RESOLVED-FIXED as the resolver of that issue. \\ \hline 
%DIF < 				\textbf{r:} The greater the experience of the resolver, the
%DIF < 				greater the likelihood that his/her code will be
%DIF < 				delivered faster~\cite{shihab2010predicting}.
%DIF < 			\end{tabular}
%DIF < 			\\ \cline{2- 4}
%DIF < 			\multicolumn{ 1}{r}{} & Resolver integration & Numeric &
%DIF < 			\begin{tabular}{p{8.7cm}}
%DIF < 				\textbf{d:} The median in days of the previously delivered addressed issues
%DIF < 				that were addressed by a particular resolver.
%DIF < 				\\ \hline 
%DIF < 				\textbf{r:} If a particular resolver usually address issues that are
%DIF < 				delivered quickly, his/her future addressed issues might be delivered
%DIF < 				quickly as well.
%DIF < 			\end{tabular}
%DIF < 			\\ \hline
%DIF < 			\multicolumn{ 1}{r}{\textbf{Issue}} & Stack trace attached & Boolean &
%DIF < 			\begin{tabular}{p{8.7cm}}
%DIF < 				\textbf{d:} We verify if the issue report has a stack trace attached in its description. \\ \hline 
%DIF < 				\textbf{r:} A stack trace attached may provide useful information regarding
%DIF < 				the cause of the issue, which may quicken the integration of
%DIF < 				the addressed issue~\cite{schroter2010stack}. 
%DIF < 			\end{tabular}
%DIF < 			\\ \cline{2- 4}
%DIF < 			\multicolumn{ 1}{r}{} & Severity & Nominal &
%DIF < 			\begin{tabular}{p{8.7cm}}
%DIF < 				\textbf{d:} The severity level of the issue report. Issues with higher
%DIF < 				severity levels (\eg blocking) might be delivered faster than other issues.
%DIF < 				\\ \hline 
%DIF < 				\textbf{r:} Panjer observed that the severity of an issue has a large effect
%DIF < 				on its time to be addressed in the Eclipse project~\cite{Panjer2007}.
%DIF < 			\end{tabular}
%DIF < 			\\ \cline{2- 4}
%DIF < 			\multicolumn{ 1}{r}{} & Priority & Nominal &
%DIF < 			\begin{tabular}{p{8.7cm}}
%DIF < 				\textbf{d:} The priority level of the issue report. Issues with higher
%DIF < 				priority levels (\eg P1) might be delivered faster than other issues.
%DIF < 				\\ \hline 
%DIF < 				\textbf{r:} Higher priority issues will likely be delivered before lower
%DIF < 				priority issues.
%DIF < 			\end{tabular}
%DIF < 			\\ \cline{2- 4}
%DIF < 			\multicolumn{ 1}{r}{} & Description size & Numeric &
%DIF < 			\begin{tabular}{p{5.6cm}}
%DIF < 				\textbf{d:} The number of words in the description of the issue. 
%DIF < 				\\ \hline 
%DIF < 				\textbf{r:} Issues that are well described might be more easy to integrate
%DIF < 				than issues that are difficult to understand.
%DIF < 			\end{tabular}
%DIF < 			\\ \hline
%DIF < 		\end{tabular}
%DIF < \end{table}
%DIF < 
%DIF < \begin{figure}[t!]
%DIF < 	\centering
%DIF < 	\includegraphics[width=\columnwidth,keepaspectratio]
%DIF < 	{chapters/chapter5/figures/rq3/model_construction.pdf}
%DIF < 	\caption{Overview of the process that we use to build our explanatory models.}
%DIF < 	\label{fig:model_construction}
%DIF < \end{figure}
%DIF < 
%DIF < \begin{table}[!t]
%DIF < 	\footnotesize
%DIF < 	\caption{Metrics that are used in our explanatory models (Project family).
%DIF < 	\label{tbl:factors2}}
%DIF < 	\centering
%DIF < 		\begin{tabular}{rp{2cm}lp{9cm}}
%DIF < 			\hline
%DIF < 			\multicolumn{1}{c}{\textbf{Family}} &
%DIF < 			\multicolumn{1}{c}{\textbf{Metrics}} & \multicolumn{1}{c}{\textbf{Value}} &
%DIF < 			\multicolumn{1}{c}{\textbf{Definition (d)$\vert$Rationale (r)}}
%DIF < 			\\ \hline
%DIF < 			\multicolumn{ 1}{r}{\textbf{Project}} & Queue rank & Numeric &
%DIF < 			\begin{tabular}{p{8.7cm}}
%DIF < 				\textbf{d:} A rank number that represents the
%DIF < 				moment at which an issue is
%DIF < 				addressed compared to other addressed issues in the backlog. For instance,
%DIF < 				in a backlog that contains $500$ issues, the first addressed issue has
%DIF < 				a rank of~1, while the last addressed issue has
%DIF < 				a rank of 500.
%DIF < 				\\ \hline 
%DIF < 				\textbf{r:} An issue with a high \textit{queue rank} is a recently
%DIF < 				addressed issue. An addressed issue might be delivered faster/slower
%DIF < 				depending of its rank.
%DIF < 			\end{tabular}
%DIF < 			\\ \cline{2- 4}
%DIF < 			\multicolumn{ 1}{r}{} & Cycle queue rank & Numeric &
%DIF < 			\begin{tabular}{p{8.7cm}}
%DIF < 				\textbf{d:} A rank number that represents the
%DIF < 				moment at which an issue is
%DIF < 				addressed compared to other addressed issues of the same release cycle. For
%DIF < 				example, in a release cycle that contains $300$ addressed issues, the first
%DIF < 				addressed issue has a rank of $1$, while the
%DIF < 				last one has a rank of $300$.
%DIF < 				\\ \hline 
%DIF < 				\textbf{r:} An issue with a high \textit{cycle queue rank} is a recently
%DIF < 				addressed issue compared to the others of the same release cycle. An issue
%DIF < 				addressed close to the upcoming release might be delivered faster.
%DIF < 			\end{tabular}
%DIF < 			\\ \cline{2- 4}
%DIF < 			\multicolumn{ 1}{r}{} & Queue position & Numeric &
%DIF < 			\begin{tabular}{p{8.7cm}}
%DIF < 				\textbf{d:} $\frac{\text{queue rank}}{\text{all addressed issues}}$. The
%DIF < 				\textit{queue rank} is divided by all the issues that are addressed by the
%DIF < 				end of the next release. A \textit{queue position} close to 1 indicates that the
%DIF < 				issue was addressed recently compared to others in the backlog.
%DIF < 				\\ \hline 
%DIF < 				\textbf{r:} An addressed issue
%DIF < 				might be delivered faster/slower depending of its position.
%DIF < 			\end{tabular}
%DIF < 			\\ \cline{2- 4}
%DIF < 			\multicolumn{ 1}{r}{} & Cycle queue position & Numeric &
%DIF < 			\begin{tabular}{p{8.7cm}}
%DIF < 				\textbf{d:} $\frac{\text{cycle queue rank}}{\text{addressed issues of the
%DIF < 				current cycle}}$. The \textit{cycle queue rank} is divided by all of the
%DIF < 				addressed issues of the release cycle. A \textit{cycle
%DIF < 				queue position} close to 1 indicates that the issue was addressed recently
%DIF < 				in the release cycle.
%DIF < 				\\ \hline 
%DIF < 				\textbf{r:}  An issue addressed close
%DIF < 				to a upcoming release might be delivered faster. 
%DIF < 			\end{tabular}
%DIF < 			\\ \hline
%DIF < 		\end{tabular}
%DIF < \end{table}
%DIF < 
%DIF < \begin{table}[!t]
%DIF < 	\footnotesize
%DIF < 	\caption{Metrics that are used in our explanatory models (Process family).
%DIF < 	\label{tbl:factors3}}
%DIF < 	\centering
%DIF < 		\begin{tabular}{rp{2cm}lp{9cm}}
%DIF < 			\hline
%DIF < 			\multicolumn{1}{c}{\textbf{Family}} &
%DIF < 			\multicolumn{1}{c}{\textbf{Metrics}} & \multicolumn{1}{c}{\textbf{Value}} &
%DIF < 			\multicolumn{1}{c}{\textbf{Definition (d)$\vert$Rationale (r)}}
%DIF < 			\\ \hline
%DIF < 			\multicolumn{ 1}{r}{\textbf{Process}} & Number of Impacted Files & Numeric & 
%DIF < 			\begin{tabular}{p{8.7cm}}
%DIF < 				\textbf{d:} The number of files that are linked to an issue report. \\ \hline 
%DIF < 				\textbf{r:} A delivery delay might be
%DIF < 				related to a high number of impacted files 
%DIF < 				because more effort would be required to properly integrate the modifications 
%DIF < 				\cite{Jiang2013}. 
%DIF < 			\end{tabular}
%DIF < 			\\ \cline{ 2- 4}
%DIF < 			\multicolumn{ 1}{r}{} & Churn & Numeric &
%DIF < 			\begin{tabular}{p{8.7cm}}
%DIF < 				\textbf{d:} The sum of added lines plus the sum
%DIF < 				of deleted lines to address the issue.\\ \hline 
%DIF < 				\textbf{r:} A higher churn suggests that a great amount of work was required
%DIF < 				to address the issue, and hence, verifying the impact of integrating the
%DIF < 				modifications may also be difficult~\cite{Jiang2013,Nagappan2005}.
%DIF < 			\end{tabular}
%DIF < 			\\ \cline{2- 4}
%DIF < 			\multicolumn{ 1}{r}{} & Fix time & Numeric &
%DIF < 			\begin{tabular}{p{8.7cm}}
%DIF < 				\textbf{d:} Number of days between the date when
%DIF < 				the issue was
%DIF < 				triaged and the
%DIF < 				date that it was addressed~\cite{Giger2010}. \\ \hline 
%DIF < 				\textbf{r:} If an issue is addressed quickly, it may have a better chance to
%DIF < 				be delivered faster.
%DIF < 			\end{tabular}
%DIF < 			\\ \cline{ 2- 4}
%DIF < 			\multicolumn{ 1}{r}{} & Number of activities & Numeric &
%DIF < 			\begin{tabular}{p{8.7cm}}
%DIF < 				\textbf{d:} An activity is an entry in the issue's history. \\ \hline 
%DIF < 				\textbf{r:} A high number of activities might indicate that much work was
%DIF < 				required to address the issue, which may impact the integration of the
%DIF < 				issue into a release~\cite{Jiang2013}.
%DIF < 			\end{tabular}
%DIF < 			\\ \cline{ 2- 4}
%DIF < 			\multicolumn{ 1}{r}{} & Number of comments & Numeric & 
%DIF < 			\begin{tabular}{p{8.7cm}}
%DIF < 				\textbf{d:} The number of comments of an issue report. \\ \hline 
%DIF < 				\textbf{r:} A large number of comments might indicate the importance of an
%DIF < 				issue or the difficulty to understand it~\cite{Giger2010}, which might
%DIF < 				impact the delivery delay~\cite{Jiang2013}.
%DIF < 			\end{tabular}
%DIF < 			\\ \cline{ 2- 4}
%DIF < 			\multicolumn{ 1}{r}{} & Interval of comments & Numeric &
%DIF < 			\begin{tabular}{p{8.7cm}}
%DIF < 				\textbf{d:} The sum of the time intervals (hour) between
%DIF < 				comments divided by the total number of comments of an issue report. \\ \hline 
%DIF < 				\textbf{r:} A short \textit{interval of comments} indicates that an intense
%DIF < 				discussion took place, which suggests that the issue is important. Hence,
%DIF < 				such an issue may be delivered faster.   
%DIF < 			\end{tabular}
%DIF < 			\\ \cline{ 2- 4}
%DIF < 			\multicolumn{ 1}{r}{} & Number of tosses & Numeric &
%DIF < 			\begin{tabular}{p{8.7cm}}
%DIF < 				\textbf{d:} The number of times that the assignee has changed. \\ \hline 
%DIF < 				\textbf{r:} Changes in the issue assignee might indicate that more than one
%DIF < 				developer have worked on the issue. Such issues may be more difficult to
%DIF < 				integrate, since different expertise from different developers might 
%DIF < 				be required~\cite{Jeong2009,Jiang2013}.  
%DIF < 			\end{tabular}
%DIF < 			\\ \hline
%DIF < 		\end{tabular}
%DIF < \end{table}
%DIF < 
%DIF < For \hyperref[ch5:rq3]{RQ3}, we build explanatory models (\ie logistic
%DIF < regression models) for the traditional and rapid releases data using the metrics
%DIF < that are presented in
%DIF < \hyperref[tbl:factors1]{Tables}~\ref{tbl:factors1},~\ref{tbl:factors2}, and
%DIF < \ref{tbl:factors3}. We model our response variable $Y$ as $Y=1$ for addressed
%DIF < issues that are delayed, \ie had their delivery prevented in at least one
%DIF < release\cite{costa2014empirical} and $Y=0$ otherwise. Hence, our models are
%DIF < intended to explain why a given addressed issue has a delayed delivery (\ie
%DIF < $Y=1$).
%DIF < 
%DIF < We follow the guidelines of Harrell~Jr.~\cite{harrell2001regression} for
%DIF < building explanatory regression models.
%DIF < \hyperref[fig:model_construction]{Figure}~\ref{fig:model_construction} provides
%DIF < an overview of the process that we use to build our models. First, we estimate
%DIF < the budget of degrees of freedom that we can spend on our models while having a
%DIF < low risk of overfitting (\ie producing a model that is too specific to the
%DIF < training data to be useful when applied to other unseen data). Second, we check for
%DIF < metrics that are highly correlated using Spearman rank correlation tests
%DIF < $(\rho)$ and we perform a redundancy analysis to remove any redundant metrics
%DIF < before building our explanatory models. 
%DIF < 
%DIF < We then assess the fit of our models using the ROC area and the Brier score. The
%DIF < ROC area is used to evaluate the degree of discrimination that is achieved by
%DIF < a model. The ROC values range between 0 (worst) and 1 (best). An area greater
%DIF < than 0.5 indicates that the explanatory model outperforms na\"{i}ve random
%DIF < guessing models. The Brier score is used to evaluate the accuracy of
%DIF < probabilistic predictions. This score measures the mean squared difference
%DIF < between the probability of delay assigned by our models for a particular issue
%DIF < $I$ and the actual outcome of $I$ (\ie whether $I$ is actually delayed or not).
%DIF < Hence, the lower the Brier score, the more accurate the probabilities that are
%DIF < produced by a model.
%DIF < 
%DIF < Next, we assess the stability of our models by computing the
%DIF < \textit{optimism-reduced} ROC area and Brier score~\cite{efron1986biased}. The
%DIF < optimism of each metric is computed by selecting a bootstrap sample to
%DIF < fit a model with the same degrees of freedom of the original model. The model
%DIF < that is trained using the bootstrap sample is applied both on the bootstrap and original
%DIF < samples (ROC and Brier scores are computed for each sample). The optimism is the
%DIF < difference in the ROC area and Brier score of the bootstrap sample and original
%DIF < sample. This process is repeated 1,000 times and the average optimism is
%DIF < computed. Finally, we obtain the \textit{optimism-reduced} scores by subtracting
%DIF < the average optimism from the initial ROC area and Brier score
%DIF < estimates~\cite{efron1986biased}.
%DIF < 
%DIF < We evaluate the impact of each metric on the fitted models using
%DIF < Wald $\chi^2$ maximum likelihood tests. The larger the $\chi^2$ value, the
%DIF < larger the impact that a particular metric has on our explanatory models'
%DIF < performance. We also study the relationship that our metrics 
%DIF < share with the likelihood of delivery delay. To do so, we plot the change in
%DIF < the estimated probability of delay against the change in a given metric
%DIF < while holding the other metrics constant at their median values using the
%DIF < \code{Predict} function of the \code{rms} package~\cite{harrell2001regression}. 
%DIF < 
%DIF < We also plot nomograms~\cite{iasonos2008build,harrell2001regression} to evaluate
%DIF < the impact of the metrics in our models. Nomograms are user-friendly charts that
%DIF < visually represent explanatory models. For instance,
%DIF < \hyperref[fig:nomogram_trad]{Figure}~\ref{fig:nomogram_trad} shows the nomogram
%DIF < of the model that we fit for the rapid release data. The higher the number of
%DIF < points that are assigned to an explanatory metric on the $x$ axis (\eg 100
%DIF < points are assigned to \textit{comments} in rapid releases), the larger the
%DIF < effect of that metric in the explanatory model. We compare which metrics are
%DIF < more important in both traditional and rapid releases in order to better
%DIF < understand the differences between these release strategies.
%DIF < 
%DIF < 
%DIF < \subsection{Methodology of Study 4}
%DIF < 
%DIF < In this study, we qualitatively analyze the delivery delay phenomena by
%DIF < surveying and interviewing the team members of our subject projects.
%DIF < 
%DIF < \subsection{Subjects of Study 4}
%DIF < 
%DIF < We analyze the Firefox, ArgoUML, and Eclipse (JDT) projects. We naturally choose
%DIF < these projects, since this qualitative study is intended to complement our prior
%DIF < quantitative analyses that we performed in those projects. We provide a brief
%DIF < description of each subject project below (we have already provided a detailed
%DIF < description in \hyperref[ch4:sec:subjects]{Section}~\ref{ch4:sec:subjects}).
%DIF < 
%DIF < ArgoUML is an open source UML modeling tool. ArgoUML provides support for all of
%DIF < the UML 1.4 diagrams. At the time that we perform this study, ArgoUML was
%DIF < downloaded 80,000 times worldwide.\smartfoot{\url{http://argouml.tigris.org}}
%DIF < ArgoUML uses the IssueZilla ITS to record its issue
%DIF < reports.\smartfoot{\url{http://argouml.tigris.org/project_bugs.html}}
%DIF < 
%DIF < Eclipse is a popular {\em Integrated Development Environment} (IDE) that is
%DIF < famous for its support for the Java programming
%DIF < language.\smartfoot{\url{https://eclipse.org/}} We study the {\em Java
%DIF < Development Tools} (JDT) project of the Eclipse
%DIF < Foundation.\smartfoot{\url{https://projects.eclipse.org/projects/eclipse.jdt}}
%DIF < The JDT project provides the Java perspective for the Eclipse IDE, which
%DIF < includes a number of views, editors, wizards, and builders. 
%DIF < 
%DIF < ArgoUML and Eclipse (JDT) adopt a traditional release cycle when compared to the
%DIF < Firefox project. For instance, the median duration of release cycles that we
%DIF < study for the ArgoUML and Eclipse (JDT) projects are 180 and 112 days,
%DIF < respectively (see \hyperref[ch:study12]{Chapter}~\ref{ch:study12}). While we are
%DIF < able to study the perceived impact of the shift between release strategies when
%DIF < surveying the participants of the Firefox project, we study the opinion of the
%DIF < ArgoUML and Eclipse participants about how that impact would be on their
%DIF < projects.
%DIF < 
%DIF < \subsection{Data collection for Study 4}\label{ch5:datacollection2}
%DIF < 
%DIF < \begin{table}[t!]
%DIF < 	\centering
%DIF < 	\footnotesize
%DIF < 	\caption{\textbf{Survey questions (excerpt).} Each horizontal line indicates a page break.
%DIF < 	\label{tbl:survey}}
%DIF < 		\begin{tabular}{p{0.95\textwidth}}
%DIF < 			\hline 
%DIF < 			\textbf{1.} For how long have you been developing software? {\em (dropdown)}\tabularnewline
%DIF < 			\textbf{2.} For how long have you worked in the (Firefox/ArgoUML/Eclipse) project?
%DIF < 			{\em (dropdown)}\tabularnewline
%DIF < 			\textbf{3.} How would you describe your roles in the software development of
%DIF < 			the Firefox/ArgoUML/Eclipse project? (e.g., developer, tester, release manager, etc.)
%DIF < 			{\em (text box)}\tabularnewline
%DIF < 			\textbf{4.} In your opinion, what motivates a development team to shift from
%DIF < 			a traditional release cycle (e.g., a release every 9 to 18 months)
%DIF < 			to a rapid release cycle (e.g., a release every 6
%DIF < 			weeks)? {\em (text box)}\tabularnewline
%DIF < 			\textbf{5.} In this survey, we consider that an issue is completed when it
%DIF < 			is implemented and tested, i.e., it is ready to be delivered. Do
%DIF < 			you remember an issue that the development team completed work on,
%DIF < 			but was not delivered to end users through the next possible release?
%DIF < 			Can you tell us what caused the delivery delay of this issue in your
%DIF < 			opinion? {\em (text box)}\tabularnewline
%DIF < 			\textbf{6.} In your experience, how common are the cases in which completed
%DIF < 			issues (issues that are implemented and tested) are omitted from the
%DIF < 			next possible release?\tabularnewline
%DIF < 			\textbf{7.} Who decides when a completed issue is delivered into an official
%DIF < 			release in your team? {\em (text box)}\tabularnewline
%DIF < 			\textbf{8.} In your opinion, when is the delivery of a completed issue to the
%DIF < 			end user considered to be delayed in your project? {\em (text box)}\tabularnewline
%DIF < 			\hline 
%DIF < 			\textbf{9.} In your opinion, is it frustrating to users when a completed issue
%DIF < 			skips one or more releases? Why? {\em (text box)}\tabularnewline
%DIF < 			\textbf{10.} Is it frustrating for the team members when a completed issue
%DIF < 			skips one or more releases? Why? {\em (text box)}\tabularnewline
%DIF < 			\hline 
%DIF < 			\textbf{11.} Assuming that an issue is completed today (implementation and
%DIF < 			testing are completed), what reasons can you think of for the issue
%DIF < 			not to be delivered to end users in the next release? {\em (text box)}\tabularnewline
%DIF < 			\hline 
%DIF < 			\textbf{12.} What can team members do to avoid the delivery delay of completed
%DIF < 			issues? {\em (text box)}\tabularnewline
%DIF < 			\textbf{13.} To what extent do you agree that the characteristics listed in
%DIF < 			the table below are related to the delivery delay of a
%DIF < 			completed issue?\\
%DIF < 			- The reporter of an addressed issue, the resolver of an
%DIF < 			addressed issue, the priority level, the severity level,
%DIF < 			number of comments, number of modified files, number of
%DIF < 			lines of code, the time at which an issue was addressed
%DIF < 			during the release cycle.
%DIF < 			{\em (5-point Likert scale for each option)}\tabularnewline
%DIF < 			\hline 
%DIF < 			\textbf{14-Firefox.} Have you worked in both traditional and rapid release cycles of
%DIF < 			the Firefox project? {\em (yes/no)}\tabularnewline
%DIF < 			\textbf{15-Firefox.} In your opinion, how much impact does a rapid release cycle have
%DIF < 			on the time to deliver completed issues for end users?
%DIF < 			{\em (text box)}\tabularnewline
%DIF < 			\textbf{16-Firefox.} Did your project evaluate the shift to rapid release cycles? If
%DIF < 			so, how? {\em (text box)}\tabularnewline
%DIF < 			\textbf{14-Others.} Do you have experience working on a
%DIF < 			rapid release cycle in any other project? {\em (yes/no)}\tabularnewline
%DIF < 			\textbf{15-Others.} In your opinion, what would be the
%DIF < 			impact of shifting to a rapid release cycle (e.g., a
%DIF < 			release every 6 weeks rather than a release every 9 to
%DIF < 			18 months) on the delay to deliver completed issues, in
%DIF < 			your project? {\em (text box)}\tabularnewline
%DIF < 			\textbf{16-Others.} If your project had shifted from a
%DIF < 			traditional to a rapid release cycle, how would you
%DIF < 			evaluate if this shift benefited your project? {\em (text box)}
%DIF < 		\end{tabular}
%DIF < \end{table}
%DIF < 
%DIF < To collect the data to perform our qualitative study, we design a web-based
%DIF < survey that was sent to 780 participants of the Firefox, Eclipse (JDT), and
%DIF < ArgoUML projects. We sent our survey to 513 Firefox, 184 Eclipse (JDT), and 83
%DIF < ArgoUML participants. We gather developer e-mails from the respective
%DIF < developer mailing list archives of the subject projects. We consider e-mail
%DIF < addresses from messages that were sent in the past 4 years. To
%DIF < encourage participation, we provided gift cards to a random subset of the
%DIF < respondents who answered all of the questions of our surveys.
%DIF < 
%DIF < Our survey is based on the two major {\em themes} that are investigated in this
%DIF < thesis. The first {\em theme} is about
%DIF < delivery delay in general, while the second {\em theme} is focused on the
%DIF < impact of switching to a rapid release cycle on the delivery delay (see
%DIF < \hyperref[fig:thesis_overview]{Figure}~\ref{fig:thesis_overview}). In
%DIF < \hyperref[tbl:survey]{Table}~\ref{tbl:survey}, we highlight a subset of the
%DIF < questions of our survey. Each horizontal line represents a page break in the
%DIF < survey. Our complete surveys are available in
%DIF < \hyperref[appendix:a]{Appendices}~\ref{appendix:a},~\ref{appendix:b},~and~\ref{appendix:c}.
%DIF < The first three questions collect demographic information. Questions \#5-13
%DIF < belong to the general delivery delay {\em theme}, while questions \#4,
%DIF < \#14-16 belong to the impact of switching to a rapid release cycle {\em theme}.
%DIF < We placed one question of the second {\em theme} early in the survey to mitigate
%DIF < bias in the responses about the motivation to switch to a rapid release cycle.
%DIF < Finally, questions \#14-16 are different for the Firefox project, since the
%DIF < other projects did not shift from a traditional to a rapid release cycle. 
%DIF < 
%DIF < In total, we receive 37 responses (5\% response rate), of which 25 responses
%DIF < come from Firefox participants, 9 from Eclipse participants, and 3 from ArgoUML
%DIF < participants. We also conduct follow-up interviews with four of the Firefox
%DIF < participants to gather deeper insights into their responses. Our interviews are
%DIF < semi-structured and our goal was to clarify the responses of our survey and
%DIF < collect more details about specific cases of delivery delays for addressed
%DIF < issues.  
%DIF < 
%DIF < \subsection{Research Approach of Study~4}
%DIF < 
%DIF < Given the exploratory nature of our qualitative analysis, we use methods from
%DIF < {\em Grounded Theory}~\cite{groundtheorybook}. The author of this thesis and
%DIF < co-author~\#1 independently conduct three sessions of open coding of the
%DIF < responses to open-ended questions (one session for each RQ). In the following,
%DIF < the codes that were generated are shared and merged into a new set of codes. The
%DIF < co-author~\#2 reviews the set of codes and adds additional entries to the final
%DIF < set of codes.  At the end of the process, we achieve 175 unique codes. Finally,
%DIF < we used axial coding to find higher level conceptual themes to answer our RQs.
%DIF < 
%DIF < \begin{table}
%DIF < 	\footnotesize
%DIF < 	\centering
%DIF < 	\caption{Participant range per subject project.
%DIF < 		\label{tbl:participants}
%DIF < 	}
%DIF < 	\begin{tabular}{lc}
%DIF < 		\hline 
%DIF < 		\textbf{Project} & \textbf{Participant range}\tabularnewline
%DIF < 		\hline 
%DIF < 		\hline 
%DIF < 		Firefox & F01--F25\tabularnewline
%DIF < 		\hline 
%DIF < 		Eclipse & E26--E34\tabularnewline
%DIF < 		\hline 
%DIF < 		ArgoUML & A35--A37\tabularnewline
%DIF < 		\hline 
%DIF < 	\end{tabular}
%DIF < \end{table}
%DIF < 
%DIF < When reporting the results of \hyperref[ch5:rq4]{RQ4}-\hyperref[ch5:rq6]{RQ6},
%DIF < we indicate in superscript the number of participants that mentioned a
%DIF < particular code that emerged during the qualitative analysis. These numbers do
%DIF < not necessarily indicate the importance of a given code, since they were coded
%DIF < based on the received responses rather than scored by participants. Also, we
%DIF < mention quotes from the interviews when necessary to provide more detail about
%DIF < the results and to allow for traceability through links to raw data. Finally,
%DIF < \hyperref[tbl:participants]{Table}~\ref{tbl:participants} shows the IDs of the
%DIF < participants that we use while reporting results.
%DIF < 
%DIF < Finally, we also perform quantitative analyses of the responses to Likert-scale
%DIF < questions. First, we check if the factors that are listed in {\em question~\#13}
%DIF < are significantly different using the ranks (responses) that are assigned to
%DIF < each factor. We use a Kruskal Wallis test~\cite{kruskal1952use} to check if
%DIF < there is a statistically significant difference between the ranks assigned to
%DIF < the factors. The Kruskal Wallis test is the non-parametric equivalent of the
%DIF < ANOVA test~\cite{fisher1925statistical} to check if there are statistically
%DIF < significant differences when comparing three or more distributions. Since
%DIF < Kruskal Wallis does not indicate which factor has statistically different values
%DIF < with respect to others, we use the Dunn test~\cite{dunn1964multiple} to perform
%DIF < specific comparisons. For example, the Dunn test indicates if the ranks that are
%DIF < assigned to the {\em number of comments} metric are statistically different when
%DIF < compared to the ones that are assigned to the {\em number of modified files}
%DIF < metric. We use the Bonferroni correction~\cite{dunn1961multiple} on the obtained
%DIF < $p$ values to account for the multiple comparisons that we perform between each
%DIF < of the factors that are listed in {\em question~\#13}. 
%DIF < 
%DIF < Additionally, we correlate the ranks that are assigned to the factors in {\em
%DIF < question~\#13} with the experience of the participants ({\em question~\#1}). To
%DIF < do that, we use Spearman rank {$\rho$} correlation~\cite{spearman1904proof},
%DIF < which is used to measure the statistical dependence between the ranks of two
%DIF < variables. Finally, we also correlate the experience of the participants with
%DIF < the perception of the frequency of delivery delay that happens in the studied
%DIF < projects ({\em question~\#6}).
%DIF < 
%DIF < \subsection{Exploratory Analysis}\label{subsubsec:exploratory}
%DIF < 
%DIF < \begin{figure}
%DIF < 	\centering
%DIF < 	\includegraphics[width=0.80\textwidth,keepaspectratio] 
%DIF < 	{chapters/chapter5/figures/demographic_experience.pdf}
%DIF < 	\caption{Software development experience of the participants.}
%DIF < 	\label{fig:demographics_experience}
%DIF < \end{figure}
%DIF < We present a exploratory analysis of the data that we collect from the responses
%DIF < of the participants.
%DIF < \hyperref[fig:demographics_experience]{Figure}~\ref{fig:demographics_experience}
%DIF < shows the experience of the participants. We collect this data from {\em
%DIF < question~\#1}. The options range from ``0 years'' to ``10 or more years''. We
%DIF < observe that 62\% ($\frac{23}{37}$) of the participants have ``10 or more
%DIF < years'' of software development experience. 
%DIF < \begin{figure}
%DIF < 	\centering
%DIF < 	\includegraphics[width=0.80\textwidth,keepaspectratio] 
%DIF < 	{chapters/chapter5/figures/demographic_experience_project.pdf}
%DIF < 	\caption{Development experience of the participants in the
%DIF < 	respective project.}
%DIF < 	\label{fig:demographics_experience_project}
%DIF < \end{figure}
%DIF < Furthermore,
%DIF < \hyperref[fig:demographics_experience_project]{Figure}~\ref{fig:demographics_experience_project}
%DIF < shows the experience of the participants related to the specific project that
%DIF < they are representing. We collect this data from {\em question~\#2} and the
%DIF < options range from ``0 years'' to ``5 or more years''. 51\%  ($\frac{19}{37}$)
%DIF < of the participants have 4 or more years of experience. 
%DIF < \begin{figure}
%DIF < 	\centering
%DIF < 	\includegraphics[width=0.80\textwidth,keepaspectratio] 
%DIF < 	{chapters/chapter5/figures/demographic_rapid_releases.pdf}
%DIF < 	\caption{Experience of the participants with respect to rapid release
%DIF < 	cycles.}
%DIF < 	\label{fig:demographics_rapid_releases}
%DIF < \end{figure}
%DIF < Moreover,
%DIF < \hyperref[fig:demographics_rapid_releases]{Figure}~\ref{fig:demographics_rapid_releases}
%DIF < shows how many participants have experience in working on rapid release cycles
%DIF < ({\em question~\#14}). We note that 57\% ($\frac{21}{37}$) of the participants
%DIF < have some experience with rapid release cycles.
%DIF < \begin{figure}
%DIF < 	\centering
%DIF < 	\includegraphics[width=0.80\textwidth,keepaspectratio] 
%DIF < 	{chapters/chapter5/figures/roles_demographics.pdf}
%DIF < 	\caption{An overview of the roles of the participants. One participant
%DIF < 	may have more than one role.}
%DIF < 	\label{fig:roles_demographics}
%DIF < \end{figure}
%DIF < \hyperref[fig:roles_demographics]{Figure}~\ref{fig:roles_demographics} shows the
%DIF < team roles that the participants classified themselves as ({\em question~\#3}).
%DIF < The majority of the participants consider themselves as ``developers'' and
%DIF < ``testers''. Since one participant can occupy several roles, the numbers that
%DIF < are shown in
%DIF < \hyperref[fig:roles_demographics]{Figure}~\ref{fig:roles_demographics} represent
%DIF < the frequency that a role was cited rather than the number of participants.
%DIF < \begin{figure}
%DIF < 	\centering
%DIF < 	\includegraphics[width=0.80\textwidth,keepaspectratio] 
%DIF < 	{chapters/chapter5/figures/delay_perception.pdf}
%DIF < 	\caption{Participants' perception on how frequent is delivery delay. The data is grouped by proportions of
%DIF < 		how many addressed issues are included in the next possible
%DIF < 		release. This data refers to the responses to {\em question~\#6}.}
%DIF < 	\label{fig:delay_perception}
%DIF < \end{figure}
%DIF < Finally, we observe that the majority of the participants perceive delivery delay as an unusual event rather than
%DIF < typical (see \hyperref[fig:delay_perception]{Figure}~\ref{fig:delay_perception}). For instance, 14 of the Firefox
%DIF < participants think that 90\% of the issues are included in the next possible release.
%DIF < 
%DIF < In our analyses to answer \hyperref[ch5:rq4]{RQ4}-\hyperref[ch5:rq6]{RQ6}, we
%DIF < attempt to correlate the rating of factors that are provided in {\em
%DIF < question~\#13} with the data that is presented in this preliminary analysis.
%DIF < 
%DIF < \subsection{Research Questions of Study~4}
%DIF < 
%DIF < We present the three research questions that are addressed in
%DIF < \hyperref[st:study4]{Study}~\ref{st:study4} below.
%DIF < 
%DIF < 
%DIF < \subsubsection*{\textit{RQ4: What are developers' perceptions as to why integration
%DIF < delays occur?}}\label{ch5:rq4}
%DIF < 
%DIF < \subsubsection*{RQ4: Motivation}
%DIF < 
%DIF < To the best of our knowledge, there is no prior work that qualitatively studies
%DIF < delivery delay. Qualitative studies are important to detect phenomena that are
%DIF < difficult to uncover quantitatively. Our goal in this RQ is to better understand
%DIF < {\em \underline{why}} delivery delays happen. This investigation is a starting
%DIF < point to reveal new ways of mitigating delivery delays.\\
%DIF < 
%DIF < \subsubsection*{RQ4: Results}
%DIF < 
%DIF < \begin{sloppypar}
%DIF < Our findings about developers' perceptions of the causes of delivery delay is
%DIF < divided into the following themes: {\em (i) development activities}, {\em (ii)
%DIF < decision making}, {\em (iii) risk}, {\em (iv) frustration}, and {\em (v) team
%DIF < collaboration}. After discussing each theme below, we present a quantitative
%DIF < analysis of the factors that can impact delivery delay using the responses to
%DIF < {\em question~\#13} (see our \hyperref[ch5:datacollection2]{data collection}
%DIF < process).\\ 
%DIF < 
%DIF < \noindent\textit{\textbf{Theme~1---Development activities.}}\theme{th:1} 
%DIF < The number of tests that should be executed was a recurrent theme among
%DIF < participants. For instance, several participants stated that {\em additional
%DIF < testing}$^{(12)}$ should be executed in order to avoid delivery delay. {\em
%DIF < P17} states that the lack of {\em ``actual user testing beyond what QA can
%DIF < provide''} can lead to delivery delay. Additionally, according to {\em F15},
%DIF < {\em ``the most common reason is that testing was incomplete''} and according to {\em
%DIF < P19}, delivery delay may happen because {\em ``testing has been too
%DIF < narrow''.} Finally, {\em E32} voices concerns about integration testing: {\em
%DIF < ``No integration tests has been done.''} Such observations bring us back to a
%DIF < core software engineering problem of when is testing
%DIF < sufficient?~\cite{beller2015much,alghamdi2016automated}.
%DIF < 
%DIF < Other recurrent themes that emerged during our qualitative analysis are {\em
%DIF < workload}$^{(7)}$ and {\em code review}.$^{(7)}$ For example, {\em E30} states
%DIF < that {\em ``As the delayed completed issues stack up, they are harder to
%DIF < integrate (the codebase is constantly changing, merge issues might emerge).''}
%DIF < Interestingly, our statistical models in our prior work (see
%DIF < \hyperref[ch:study12]{Chapter}~\ref{ch:study12})
%DIF < indicate {\em workload}$^{(7)}$ as a metric that shares a strong relationship
%DIF < with delivery delay. As for {\em code review},$^{(7)}$ the {\em
%DIF < ``Unavailability of the lead/reviewer/[Project Management Committee] (PMC)''} is
%DIF < a reason of delivery delay that is pointed out by {\em E26}, while {\em F08}
%DIF < argues that a {\em ``prompt code reviews [may] help''} to avoid delivery
%DIF < delays~\cite{mcintosh2016emse}.\\
%DIF < 
%DIF < \noindent\textit{\textbf{Theme~2---Decision making.}}\theme{th:2} Decision making refers to the
%DIF < activities that are not directly related to software construction, but can
%DIF < influence the speed at which software is shipped. For example, how early a
%DIF < codebase should be ``frozen''? Which issues should be prioritized? The {\em
%DIF < timing}$^{(9)}$ and {\em prioritization}$^{(9)}$ are the recurrent themes in
%DIF < our survey responses. For instance, two of the participants stated that issues
%DIF < can be delayed because they are addressed {\em ``too late in the release
%DIF < cycle''} ({\em E28}) or because they were addressed in a {\em ``long release
%DIF < cycle.''} Also, {\em F12}'s opinion about how to avoid delivery delay is to
%DIF < {\em ``test [addressed issues] early using real users (\eg on the pre-release
%DIF < channels).''} Regarding {\em prioritization},$^{(9)}$ {\em E28} argues that team
%DIF < members should {\em ``try to complete most important things early in the release
%DIF < cycle''} to avoid delivery delay. Additionally, {\em F07} points out how
%DIF < re-prioritization of issues is important: {\em ``[...] prioritizing and
%DIF < re-prioritizing tasks to be sure you are building things on time [...].''}\\
%DIF < 
%DIF < \noindent\textit{\textbf{Theme~3---Risk.}}\theme{th:3} The risk that is associated with shipping
%DIF < addressed issues may generate delivery delay according to our participants.
%DIF < Among the risky addressed issues, the ones that have {\em
%DIF < compatibility}$^{(12)}$ concerns are the most recurrent in this theme. For example, when
%DIF < asked about reasons that may lead to delivery delay, {\em F12} calls
%DIF < attention to issues that {\em ``break third-party websites''} and that can
%DIF < generate {\em ``incompatibility with third-party software that users install.''}
%DIF < Another risk that is associated with delivery delay is {\em stability}.$^{(9)}$
%DIF < For instance, {\em F03} states that {\em ``when there are regressions
%DIF < noticed during Aurora/Beta cycles,''} an addressed issue will likely skip the
%DIF < upcoming official release.\\
%DIF < 
%DIF < \noindent\textit{\textbf{Theme~4---Frustration.}}\theme{th:4} Delivery delay may generate
%DIF < frustration to both users and developers of the software. The majority of users'
%DIF < frustration comes from their {\em expectation}$^{(20)}$ about the addressed
%DIF < issues. {\em F07} makes an interesting analogy to explain user frustration: {\em
%DIF < ``as a user, it's like when you are waiting your suitcase in the airport to
%DIF < come out on the belt. You know it has to be there, but you keep waiting.''}
%DIF < {\em F14} also provides another analogy: {\em ``it's like a gift for Christmas,
%DIF < but the day of Christmas is postponed.''} On the other hand, developers may get
%DIF < frustrated for other reasons than users. The greatest frustration source for
%DIF < developers is the feeling of {\em useless/unreleased work}.$^{(9)}$ According to
%DIF < {\em F09}, when an addressed issue is delayed, a developer {\em ``feels like
%DIF < [their] work is meaningless.''} {\em F04} complements {\em F09} by stating that
%DIF < {\em ``it is frustrating to work on something and not see it shipped.''} \\
%DIF < 
%DIF < \noindent\textit{\textbf{Theme~5---Collaboration with other teams.}}\theme{th:5}
%DIF < Delivery delay may also occur due to
%DIF < the overhead that is introduced when {\em collaboration}$^{(10)}$ is needed
%DIF < between teams. For example, when asked to recall a delayed addressed
%DIF < issue, {\em F23} answers that {\em ``sometimes, issues that require cross-team
%DIF < cooperation may be delayed when the issue is differently prioritized by each
%DIF < team.''} The {\em marketing}$^{(5)}$ team is mentioned recurrently when
%DIF < delivery delay occurs due to other teams' collaboration. For instance,
%DIF < according to {\em F21}, delivery delay {\em ``generally happens when
%DIF < marketing wants to make a splash.''} {\em F08} also corroborates  {\em F21} by
%DIF < stating that {\em ``product management [may] change their mind about the
%DIF < desirability of a feature, or would like to time the release of the feature with
%DIF < certain external events for marketing reasons.''}\\
%DIF < 
%DIF < \noindent\textit{\textbf{Observation~7---The time at which an issue is addressed
%DIF < during a release cycle and the issue severity are the factors that receive the
%DIF < highest ratings of importance.}}\observation{obs:7} In {\em
%DIF < question~\#13} of our survey, we ask participants to rate the degree to which a
%DIF < factor is related to delivery delay. The factors that we list are: the
%DIF < reporter, the resolver, the priority, the severity, the number of comments, the
%DIF < number of modified files, the number of modified LOC, and the time at which an
%DIF < issue was addressed during a release cycle. The responses to {\em question~\#13}
%DIF < are based on a 5-points-Likert scale, \ie participants rate factors using ranks from 1
%DIF < (strongly disagree) to 5 (strongly agree). 
%DIF < 
%DIF < \begin{figure}
%DIF < 	\centering
%DIF < 	\includegraphics[width=0.80\textwidth,keepaspectratio]
%DIF < 	{chapters/chapter5/figures/rank_frequency.pdf}
%DIF < 	\caption{Frequency of ranks per factor.}
%DIF < 	\label{fig:rank_frequency}
%DIF < \end{figure}
%DIF < 
%DIF < \begin{table}
%DIF < 	\footnotesize
%DIF < 	\centering
%DIF < 	\caption{Rating of factors related to delivery delay. The highest
%DIF < 		ratings are in bold.
%DIF < 		\label{tbl:factors}
%DIF < 	}
%DIF < 	\begin{tabular}{lr}
%DIF < 		\hline 
%DIF < 		\textbf{Factor} & \textbf{Average rating (mean)}\tabularnewline
%DIF < 		\hline 
%DIF < 		\hline 
%DIF < 		Time at which an issue is addressed during a release cycle (timing) & \textbf{4.257}\tabularnewline
%DIF < 		\hline 
%DIF < 		Severity & \textbf{4.086}\tabularnewline
%DIF < 		\hline 
%DIF < 		Priority & 3.629\tabularnewline
%DIF < 		\hline 
%DIF < 		Number of LOC & 3.571\tabularnewline
%DIF < 		\hline 
%DIF < 		Resolver & 3.441\tabularnewline
%DIF < 		\hline 
%DIF < 		Number of files & 3.314\tabularnewline
%DIF < 		\hline 
%DIF < 		Number of comments & 2.657\tabularnewline
%DIF < 		\hline 
%DIF < 		Reporter & 2.629\tabularnewline
%DIF < 		\hline 
%DIF < 	\end{tabular}
%DIF < \end{table}
%DIF < 
%DIF < In \hyperref[fig:rank_frequency]{Figure}~\ref{fig:rank_frequency}, we show the
%DIF < frequency of each rank per factor, while  we show the average rating of each
%DIF < factor in \hyperref[tbl:factors]{Table}~\ref{tbl:factors}. We observe that the
%DIF < factors that receive the highest ranks are {\em severity} and {\em timing}. This
%DIF < result is in agreement with our regression models that are presented in RQ3, in
%DIF < which {\em cycle queue rank} is one of the most influential variables
%DIF < (see \hyperref[obs:6]{Observation}~\ref{obs:6}). Indeed, during the interview, {\em
%DIF < P06} further explains that if an issue that is risky is addressed in the end of
%DIF < a release cycle, such an issue is likely to be delayed to the next cycle, so
%DIF < that it can receive additional testing.
%DIF < 
%DIF < \begin{figure}
%DIF < 	\centering
%DIF < 	\includegraphics[width=.8\textwidth,keepaspectratio]
%DIF < 	{chapters/chapter5/figures/comments_ratio.pdf}
%DIF < 	\caption{Distribution of number of comments normalized by the number of
%DIF < 	reported issues.}
%DIF < 	\label{fig:comments_ratio}
%DIF < \end{figure}
%DIF < 
%DIF < On the other hand, the factors with the lowest ranks are {\em reporter}, and
%DIF < {\em \# of comments}. We also asked our interviewees about these lower ratings.
%DIF < One of our interviewees explained that the reporter of an issue might influence
%DIF < delivery delay only in cases in which the reporter is also a Firefox
%DIF < employee. In these cases, the reporter will address the issue her/himself, which
%DIF < can speed up the shipping process.\footnote{We did not observe a statistically
%DIF < 	significant difference in delivery delays between issues that are
%DIF < addressed by the reporters themselves and issues that are addressed by a
%DIF < different team member.} As for the {\em \# of comments}, another interviewee
%DIF < clarified that there are several passionate people on bugs that can inflate the
%DIF < number of comments even if the issue is easy to ship. For each reporter, we
%DIF < normalize the number of his/her comments by the number of his/her reported issues.  We
%DIF < plot the distribution of the normalized number of comments in
%DIF < \hyperref[fig:comments_ratio]{Figure}~\ref{fig:comments_ratio}. The median
%DIF < number of comments per reported issue is 98. Indeed, we observe reporters with a
%DIF < great number of comments (\eg 500 to 10,000 comments) per reported issue.
%DIF < This result suggest that the perception of our interviewee is likely to be true.
%DIF < 
%DIF < \begin{table}
%DIF < 	\scriptsize
%DIF < 	\centering
%DIF < 	\caption{$P$-$values$ of the comparisons between factors. Values in bold are $<$ 0.05.
%DIF < 		\label{tbl:pvalues_factors}
%DIF < 	}
%DIF < 	\begin{tabular}{lrrrr}
%DIF < 		\hline 
%DIF < 		Factor x Factor & Reporter & Resolver & Priority & Severity\tabularnewline
%DIF < 		\hline 
%DIF < 		\hline 
%DIF < 		Reporter & --- & 1.2$e^{-1}$ & \textbf{1.3$e^{-2}$} & \textbf{1.4$e^{-5}$} \tabularnewline
%DIF < 		\hline 
%DIF < 		Resolver & 1.2$e^{-1}$ & --- & 1 & 1.2$e^{-1}$ \tabularnewline
%DIF < 		\hline 
%DIF < 		Priority & \textbf{1.3$e^{-2}$} & \textbf{1.3$e^{-2}$} & --- & 4.8$e^{-1}$ \tabularnewline
%DIF < 		\hline 
%DIF < 		Severity & \textbf{1.4$e^{-5}$} & 1.2$e^{-1}$ & 4.7$e^{-1}$ & ---\tabularnewline
%DIF < 		\hline 
%DIF < 		\# of Comments & 4.8$e^{-1}$ & 1.2$e^{-1}$ & \textbf{1.4$e^{-2}$} & \textbf{1.6$e^{-5}$} \tabularnewline
%DIF < 		\hline 
%DIF < 		\# of Files & 1.8$e^{-1}$ & 7.9$e^{-1}$ & 1 & 6.6$e^{-2}$ \tabularnewline
%DIF < 		\hline 
%DIF < 		\# of LOC & \textbf{3.0$e^{-2}$} & 1 & 1 & 2.8$e^{-1}$ \tabularnewline
%DIF < 		\hline 
%DIF < 		Timing & \textbf{8.0$e^{-7}$} & \textbf{3.2$e^{-2}$} & 1.8$e^{-1}$ & 1\tabularnewline
%DIF < 		\hline 
%DIF < 		\hline
%DIF < 		Factor x Factor & \# of Comments & \# of Files & \# of LOC & Timing\tabularnewline
%DIF < 		\hline 
%DIF < 		Reporter & 4.8$e^{-1}$ & 1.8$e^{-1}$ & \textbf{3.0$e^{-2}$} & \textbf{8.1$e^{-7}$} \tabularnewline
%DIF < 		\hline 
%DIF < 		Resolver & 1.2$e^{-1}$ & 7.9$e^{-1}$ & 1 & \textbf{3.2$e^{-2}$} \tabularnewline
%DIF < 		\hline 
%DIF < 		Priority & \textbf{1.4$e^{-2}$} & 1 & 1 & 1.8$e^{-1}$ \tabularnewline
%DIF < 		\hline 
%DIF < 		Severity & \textbf{1.6$e^{-5}$} & 6.6$e^{-2}$ & 2.8$e^{-1}$ & 1\tabularnewline
%DIF < 		\hline 
%DIF < 		\# of Comments & --- & 1.7$e^{-1}$ & \textbf{3.3$e^{-2}$} & \textbf{9.6$e^{-7}$} \tabularnewline
%DIF < 		\hline 
%DIF < 		\# of Files & 1.7$e^{-1}$ & --- & 1 & \textbf{1.4$e^{-2}$} \tabularnewline
%DIF < 		\hline 
%DIF < 		\# of LOC & 3.3$e^{-2}$ & 1 & --- & 1.1$e^{-1}$ \tabularnewline
%DIF < 		\hline 
%DIF < 		Timing & \textbf{9.6$e^{-7}$} & \textbf{1.4$e^{-2}$} & 1.1$e^{-1}$ & ---\tabularnewline
%DIF < 		\hline 
%DIF < 	\end{tabular}
%DIF < \end{table}
%DIF < 
%DIF < A Kruskal Wallis test indicates that the difference in ratings between metrics
%DIF < are statistically significant ($p=0.01507$).
%DIF < \hyperref[tbl:pvalues_factors]{Table}~\ref{tbl:pvalues_factors} shows the
%DIF < Bonferroni corrected $p$-$values$ of the Dunn tests. We observe that the {\em
%DIF < timing} factor has significant larger response values than all the other factors except
%DIF < the {\em severity}, {\em priority}, and {\em LOC} factors ($p<0.05$).
%DIF < 
%DIF < We also use Spearman's $\rho$ to correlate the rating of the factors with (i)
%DIF < general experience ({\em question \#1}) and (ii) project experience ({\em
%DIF < question \#2}). The only statistically significant correlation that we observe is
%DIF < between the {\em timing} factor and general experience. We achieve a negative
%DIF < correlation of -0.36 ($p=0.03235$). This result suggests that less
%DIF < experienced participants tend to report that the time at which an issue is
%DIF < addressed during a release cycle plays a more important role in delivery
%DIF < delay. One of our interviewees explains this observation by stating that {\em
%DIF < ``when an issue is addressed early in the release cycle, it should have more
%DIF < time to be tested before integration,''} which can be helpful for fixes
%DIF < from less experienced resolvers. Finally, we also correlate the responses
%DIF < to {\em question~\#6} with general and project experience. However, no
%DIF < significant correlations were found.\\
%DIF < \end{sloppypar}
%DIF < 
%DIF < \conclusionbox{Our survey participants report that the delivery of addressed
%DIF < issues may be delayed due to reasons that are related to the development
%DIF < activities, decision making, team collaboration, or risk. Moreover, delivery
%DIF < delay likely lead to user/developer frustration.}
%DIF < 
%DIF < \subsubsection*{\textit{RQ5: What are developers' perceptions of shifting to a
%DIF < rapid release cycle?}}\label{ch5:rq5}
%DIF < 
%DIF < \subsubsection*{RQ5: Motivation}
%DIF < 
%DIF < In this research question, we intend to complement our quantitative findings
%DIF < about the comparison between traditional and rapid release cycles regarding
%DIF < delivery delays. This investigation is important to gain deeper explanations as
%DIF < to why addressed issues may be delivered more quickly in traditional releases.
%DIF < Additionally, we intend to understand what are the reasons for the perceived
%DIF < success of adopting a rapid release cycle. This is also important to help
%DIF < project leaders with their decision of adopting a rapid release cycle rather
%DIF < than a traditional one. \\
%DIF < 
%DIF < \subsubsection*{RQ5: Results}
%DIF < 
%DIF < In this RQ, we study the perceptions of developers about the impact of shifting
%DIF < to a rapid release cycle. Our findings about these perceptions are organized
%DIF < along the following themes: {\em management}, {\em delivery}, and {\em
%DIF < development}. We describe each theme below.\\    
%DIF < 
%DIF < \noindent{\textit{\textbf{Theme~6---Management.}}\theme{th:6} The shift to a rapid
%DIF < release cycle has a considerable impact on release cycle management. 
%DIF < 
%DIF < The most recurrent theme in this respect is {\em flexibility}$^{(4)}$ to plan
%DIF < the scope of the releases that should be shipped.  {\em F01}'s opinion is that
%DIF < rapid releases {\em ``provide a bit more flexibility, since if an important
%DIF < issue pushed back a less important change and it misses the release cycle, it's
%DIF < not a huge deal with rapid releases.''} {\em F01}'s observation is supported by
%DIF < our observation that rapid Firefox releases tend to deliver addressed issues
%DIF < more consistently (see \hyperref[obs:2]{Observation}~\ref{obs:2}). 
%DIF < 
%DIF < Another perceived advantage of rapid release cycles are the {\em risk
%DIF < mitigation}$^{(3)}$ and {\em better prioritization}.$^{(3)}$ With respect to
%DIF < {\em risk mitigation},$^{(3)}$ {\em F07} argues that in rapid release cycles,
%DIF < the team is {\em ``able to identify issues sooner. It is easier to identify
%DIF < issues when you have only deployed 3 new commits than 100.''} As for {\em better
%DIF < prioritization},$^{(3)}$ {\em F19} explains that rapid release cycles {\em
%DIF < ``probably decreases unnecessary delays of the releases because deadline is
%DIF < closer and developers have to react faster for the pressuring issues.
%DIF < Non-critical issues gets also pushed back and don't receive useless attention
%DIF < nor create delays.''} Still on the {\em better prioritization}$^{(3)}$ matter,
%DIF < {\em F17} adds that rapid releases {\em ``provide a time box in which [the team]
%DIF < must forecast the top priority work to complete within that time frame.'' }\\
%DIF < 
%DIF < \noindent\textit{\textbf{Theme~7--- Delivery.}}\theme{th:7} The most recurrent perceived
%DIF < advantage of rapid release cycles is the {\em ``faster delivery''}$^{(33)}$
%DIF < of new functionalities. When asked about the motivation to use rapid release
%DIF < cycles, {\em F05} mentions {\em ``increasing speed of getting new features to
%DIF < users,''} while {\em F06} mentions a similar statement: {\em ``getting new features to
%DIF < users sooner.''} Interestingly, not all participants that mentioned the time to
%DIF < deliver new functionalities report that rapid releases always reduce such time.
%DIF < For {\em F22}, rapid releases {\em ``reduce the time to deliver issues to end
%DIF < users in some cases, and lengthen them in others.''} More specifically, {\em F24} 
%DIF < says that {\em ``Low priority issues (new features) take less
%DIF < time to be delivered, whereas high priority ones (important bugs) take more
%DIF < time.''} 
%DIF < 
%DIF < Another recurrent perception about rapid releases is the {\em faster user
%DIF < feedback}$^{(17)}$ due to the constant delivery of new functionalities. For
%DIF < instance, {\em E29} provides an example that {\em ``you don't find yourself
%DIF < fixing a bug that you introduced two years ago which the field only discovered on the
%DIF < release.''}\\
%DIF < 
%DIF < \noindent\textit{\textbf{Theme~8---Development activities.}}\theme{th:8}
%DIF < We do not observe a specific theme that is recurrent with respect
%DIF < to development activities. Instead, we observe a broad range of themes that are
%DIF < cited by the participants. Among such themes, we observe {\em quality},$^{(3)}$
%DIF < {\em more functionalities},$^{(2)}$ {\em better motivation},$^{(2)}$ and {\em
%DIF < better prototyping}.$^{(2)}$ Quality should be a measure of success of using
%DIF < rapid release cycles. According to {\em E26}, {\em ``quality of delivered code
%DIF < should remain the same or improve''} after switching to rapid releases. Another
%DIF < way to measure the success of a rapid release cycle is the {\em number of
%DIF < functionalities}$^{(2)}$ that are completed. {\em E34} states the following: {\em
%DIF < ``I would see if more issues were completely fixed''} as a measure of success. 
%DIF < 
%DIF < Moreover, rapid releases may also impact team members' motivation. For instance,
%DIF < {\em F06}'s opinion about why to switch to rapid release cycles is {\em ``the
%DIF < need to motivate the community via more frequent collaboration.''} Finally,
%DIF < rapid release cycles may also improve prototyping activities. For instance, {\em
%DIF < P27} argues that, by adopting rapid releases, a development team can {\em ``fix
%DIF < bugs quickly [and] prototype features, having results in few months.''}
%DIF < 
%DIF < \conclusionbox{The allure of delivering addressed
%DIF < issues more quickly to users is the most recurrent motivator of switching to a
%DIF < rapid release cycle. Moreover, the allure of improving management flexibility and quality
%DIF < of addressed issues are other advantages that are perceived by our participants
%DIF < with respect to switching to rapid release cycles.}
%DIF < 
%DIF < \subsubsection*{\textit{RQ6: To what extent do developers agree with our quantitative
%DIF < findings about delivery delay?}}\label{ch5:rq6}
%DIF < 
%DIF < \subsubsection*{RQ6: Motivation}
%DIF < 
%DIF < The main motivation for this research question is to solicit feedback about our
%DIF < quantitative findings. More specifically, we aim to understand to what extent
%DIF < our quantitative findings agree with the participants' perception of integration
%DIF < delays.
%DIF < 
%DIF < \subsubsection*{RQ6: Results}
%DIF < 
%DIF < In this research question, we investigate how our participants feel about the
%DIF < data that we collect during our prior quantitative studies
%DIF < (\hyperref[st:study1]{Studies}~\ref{st:study1}, \ref{st:study2},
%DIF < and~\ref{st:study3}). This research question
%DIF < is divided into two subsections---one for each {\em theme} that is investigated
%DIF < in this thesis---{\em (i)} delivery delay in general
%DIF < and {\em (ii)} the impact of rapid release cycles on delivery delay
%DIF < (\hyperref[fig:thesis_overview]{Figure}~\ref{fig:thesis_overview}).\\
%DIF < 
%DIF < 
%DIF < \noindent\textit{\textbf{Theme~9---delivery delay in general.}}\theme{th:9}
%DIF < In this analysis, we present the data that we collect in our prior studies
%DIF < (\hyperref[ch:study12]{Chapter}~\ref{ch:study12}) and investigate if this data
%DIF < resonates with participants' experience. We provide the methodology of our
%DIF < data-related questions to participants through a web page that is mentioned in
%DIF < our surveys (see~\hyperref[methodology:i]{Appendix}~\ref{methodology:i}).
%DIF < 
%DIF < \begin{figure}
%DIF < 	\centering
%DIF < 	\includegraphics[width=0.80\textwidth,keepaspectratio]
%DIF < 	{chapters/chapter5/figures/rq6/datasets.pdf}
%DIF < 	\caption{Proportion of addressed issues that have their integration
%DIF < 		delayed by a given number of releases. For example, 89\% of the
%DIF < 	addressed issues skip two Firefox stable releases before being shipped
%DIF < to users (this chart was already presented in
%DIF < \hyperref[ch:study12]{Chapter}~\ref{ch:study12}. }
%DIF < 	\label{fig:data-related-rq1}
%DIF < \end{figure}
%DIF < 
%DIF < \hyperref[fig:data-related-rq1]{Figure}~\ref{fig:data-related-rq1} shows the
%DIF < chart that we presented to participants. For example, 89\% of the addressed
%DIF < issues skip two Firefox stable releases before being shipped to users. The most
%DIF < recurrent themes among the responses of participants to explain this data are:
%DIF < {\em team workload}$^{(5)}$ and {\em dependency}.$^{(2)}$ Among the responses that
%DIF < are related to {\em team workload},$^{(5)}$ {\em E27} explains that {\em
%DIF < ``committers  are too busy,''} while {\em E26} argues that there might be {\em
%DIF < ``delay[s] in review[s] when the issues [are] completed,''} which can generate
%DIF < delivery delay. Regarding {\em dependency},$^{(2)}$ {\em E32}'s opinion is
%DIF < that delivery delay may happen due to {\em ``the strong connection to other
%DIF < Eclipse projects which makes integration more costly (time consuming).''}
%DIF < Furthermore, two of our interviewees ({\em F11} and {\em F23}) provide us with
%DIF < examples of why addressed issues may be delayed due to dependency problems. For
%DIF < example, {\em F23} explains during the interview that delivery delay can
%DIF < happen when there are {\em ``dependencies between projects and one of them gets
%DIF < done, but the other implementation takes a longer while.''} Another example,
%DIF < provided by {\em F23} is when {\em ``you release a bug fix but then you realize:
%DIF < 	Hey! These users are not able to use these websites anymore because web
%DIF < 	servers implement the spec in a wrong way or do some really weird things
%DIF < that are not expected.''}
%DIF < 
%DIF < Additionally, we ask participants from the Eclipse and ArgoUML projects about
%DIF < their opinion of why the data from the Firefox project behaves differently from
%DIF < theirs, \ie a larger number of releases being skipped by addressed issues. The
%DIF < most recurrent responses explain that this difference may be due to the {\em
%DIF < 	rapid release cycle}$^{(4)}$ that is adopted by the Firefox project. For
%DIF < 	example, {\em E30}'s opinion is that {\em ``on a rapid release cycle
%DIF < 		(e.g. 6 weeks for firefox), a two-release delay means 12 weeks,
%DIF < 		less than 3 months, which is still less than no delay for a fix
%DIF < 		submitted early in a project with a 6-month release cycle.''} 
%DIF < \\           
%DIF < 
%DIF < \noindent\textit{\textbf{Theme~10---Impact of switching to a rapid release
%DIF < cycle.}}\theme{th:10}
%DIF < We present the data that is shown in
%DIF < \hyperref[fig:traditional_vs_rapid]{Figure}~\ref{fig:traditional_vs_rapid} to
%DIF < the participants of the Firefox project.
%DIF < \hyperref[fig:traditional_vs_rapid]{Figure}~\ref{fig:traditional_vs_rapid}
%DIF < compares the delivery delay between traditional and rapid release cycles. We
%DIF < then ask if this result resonates with the participants' experience. More
%DIF < details about how we show this data to participants can be found in
%DIF < \hyperref[methodology:ii]{Appendix}~\ref{methodology:ii}. From the 14 responses that we received for this
%DIF < question, 5 participants explicitly disagree with our analysis, while 6
%DIF < participants explicitly agree with it. 
%DIF < 
%DIF < We interviewed two participants that disagree with the results ({\em F06} and
%DIF < {\em F09}). After providing extra explanation about our methodology and asking
%DIF < them to elaborate on their responses, we could better understand their reasons.
%DIF < {\em F06} clarifies: {\em ``I'm not surprised that there are things in that
%DIF < bucket''} (the short delays due to minor traditional releases), instead 
%DIF < {\em ``I'm surprised that there are many of them.''} In addition, {\em F09} declared
%DIF < {\em ``I misunderstood [your] question, but now it [(the data)] makes
%DIF < sense.''} With respect to the remaining participants that disagree with our
%DIF < results, they inform us that the data does not resonate with their experience. For
%DIF < instance, {\em F21} provides the following opinion {\em ``this does not resonate
%DIF < with my experience. I find the traditional model is much much slower than rapid
%DIF < release to get fixes in users hands.''} 
%DIF < 
%DIF < From the set of participants that agree with our results, two of them explain
%DIF < that the behaviour that is presented by the traditional release data is due to
%DIF < the {\em integration rush}$^{(2)}$ that happens prior to shipping. {\em F15}'s
%DIF < opinion is that {\em ``since missing a release cycle isn't a big deal, more
%DIF < features are kept from being released until they're properly polished instead of
%DIF < being rushed at the end of a long release cycle.''} {\em F22} also provides us
%DIF < with a reasonable explanation when stating that our result {\em ``makes perfect
%DIF < 	sense as issues will, unless fast-tracked or held back, be released a
%DIF < 	set quantum of time after they are completed. This is dominated by the
%DIF < 	timing of the release schedule, not by the timing of the discovery or
%DIF < fix.''}\\
%DIF < 
%DIF < \conclusionbox{The dependency of addressed issues on other projects and team
%DIF < 	workload are major perceived reasons to explain our findings about
%DIF < 	delivery delays. Moreover, participants are divided when explaining
%DIF < 	why traditional releases may have shorter delivery delays.
%DIF < 	Nevertheless, the fact that in rapid releases an integration rush is no
%DIF < 	longer needed and that additional time can be spent on polishing
%DIF < 	addressed issues emerge as main explanations as to why traditional
%DIF < 	releases can have shorter delivery delays.}

